{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0a73b1",
   "metadata": {
    "id": "fc0a73b1"
   },
   "source": [
    "# Multiclass Segmentation Metrics\n",
    "> Multiclass segmentation metrics with torchmetrics, highlighting the difference between micro, macro, and macro-imagewise metrics.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [pytorch, torchmetrics, segmentationmodels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af5b8",
   "metadata": {
    "id": "f95af5b8"
   },
   "source": [
    "In my last [post]({% post_url 2022-08-25-pet_segmentation %}) I showed how to use `torchmetrics` to implement segmentation metrics for the Oxford-IIIT pet segmentation dataset. We saw that in addition to the `average` keyword introduced in the [pet breed classification]({% post_url 2022-08-17-pet_breed_classification %}) post, the `mdmc_average` keyword is necessary to compute metrics for image data. \n",
    "\n",
    "In this post we'll dive deeper into these metrics, explaining the two choices for the `mdmc_average` parameter, including *global* and *samplewise*, as well as giving recommendations for dealing with imbalanced datasets.\n",
    "\n",
    "The examples below will look primarily at precision and $F1$ score, but note that these metrics can be replaced by recall, dice score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7068d",
   "metadata": {
    "id": "cfb7068d"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "595a4bfe",
   "metadata": {
    "id": "595a4bfe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics.functional.classification import precision, f1_score\n",
    "from torchmetrics.classification import StatScores\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set the seed for reproduciblity.\n",
    "torch.manual_seed(7)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ee35d",
   "metadata": {
    "id": "884ee35d"
   },
   "source": [
    "To better understand the metrics, we'll work with a $4$ class problem with $n = 100$ samples. Classes $0$ and $3$ will have a probability of occurence of $\\frac{1}{15}$, class $1$ will have a probability of $\\frac{2}{3}$, and class $2$ will have a probability of $\\frac{1}{5}$. We can generate data having this distribution using `torch.multinomial` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7513e786",
   "metadata": {
    "id": "7513e786"
   },
   "outputs": [],
   "source": [
    "weights = torch.tensor([1, 10, 3, 1], dtype=torch.float)\n",
    "num_classes = len(weights)\n",
    "shape = (100,1,256,256)\n",
    "size = functools.reduce(lambda x, y : x* y, shape)\n",
    "output = torch.multinomial(weights, size, replacement=True).reshape(shape)\n",
    "output[70:,:,:,:] = torch.zeros(30, *shape[1:])\n",
    "target = torch.multinomial(weights, size, replacement=True).reshape(shape)\n",
    "target[70:,:,:,:] = torch.zeros(30, *shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68295073",
   "metadata": {
    "id": "68295073"
   },
   "source": [
    "For example, a subset of the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c483999",
   "metadata": {
    "id": "4c483999",
    "outputId": "c67b961c-45ac-46f2-ad3a-f2f52b2c20b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 2, 1, 2, 2, 1, 1, 2, 2],\n",
       "         [1, 1, 1, 1, 1, 3, 1, 1, 1, 1],\n",
       "         [1, 1, 3, 2, 1, 2, 1, 1, 1, 1],\n",
       "         [0, 1, 1, 2, 3, 1, 1, 1, 1, 2],\n",
       "         [1, 0, 1, 1, 1, 1, 1, 1, 1, 3],\n",
       "         [1, 1, 1, 2, 0, 1, 1, 0, 1, 1],\n",
       "         [1, 1, 1, 0, 1, 1, 2, 1, 2, 1],\n",
       "         [2, 1, 1, 1, 2, 1, 2, 1, 3, 2],\n",
       "         [3, 1, 1, 3, 1, 2, 1, 1, 1, 1],\n",
       "         [2, 3, 0, 1, 1, 1, 1, 2, 2, 1]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,:,:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1716f85-d5a6-4ca6-b7e2-ccede655a448",
   "metadata": {},
   "source": [
    "First we can collapse the image dimensions, $H$ and $W$, and then calculate metrics as for multiclass classification. This is precisely what happens\n",
    "when we choose `mdmc_average` global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "990bda74-82a3-44f0-b725-9112ea26e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4517214596271515"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(output, target,num_classes=num_classes,average=\"macro\",mdmc_average=\"global\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019fc27-a9af-46b0-af42-9f4f1a98d644",
   "metadata": {},
   "source": [
    "For comparisons sake, in scikit-learn we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58bedb70-9266-4f27-bf53-99741f1fad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4517214441963613"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(target.reshape((-1)).numpy(),output.reshape((-1)).numpy(), average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5b4db-c9f3-4db4-85fb-5b548ef07871",
   "metadata": {},
   "source": [
    "Then the different options for `average` can be chosen, including **micro**, **macro**, and **weighted**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b81ff9-4346-4576-aa08-ae70be6b4998",
   "metadata": {},
   "source": [
    "In contrast, the image dimensions can be treated separately, which is called the **macro-imagewise** reduction:\n",
    "\n",
    "1. For each image and class the confusion table is computed over all pixels in an image.\n",
    "2. Then the metric is computed for each image and class, as if it were a binary classifier.\n",
    "3. The metrics are finally averaged over the images and classes.\n",
    "\n",
    "This is the most natural way to calculate metrics like the Jaccard index (intersection over union) for example. Unfortunately the jaccard index can't be calculated this way using `torchmetrics`. However the $F1$/Dice Score can be calculated using `torchmetrics`, and it's equivalent to the Jaccard index{% fn 1 %}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97309288-d336-489e-930b-62adfa09c74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2497853934764862"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(output, target,num_classes=num_classes,average=\"macro\",mdmc_average=\"samplewise\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fac8a0-5f7b-4c09-80d1-1230af5b668e",
   "metadata": {},
   "source": [
    "However if we calculate the $F1$ score using the segmentation models library, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a3b901-7b05-4b02-9ad9-a1a39b9ef376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47478538751602173"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode='multiclass', num_classes=num_classes)\n",
    "smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5296c-7810-4e19-b4e2-d542f4188e22",
   "metadata": {},
   "source": [
    "This is because our dataset has many images with no targets (recall that we zeroed out several images). Thus the $F1$ score\n",
    "for non-background classes reduces to $\\frac{0}{0}$. `smp` replaces occurences of $\\frac{0}{0}$ by $1$, while `torchmetrics` replaces $\\frac{0}{0}$ by $0$.\n",
    "If we pass `zero_division=0` to the segmentation models library, we get the same value as `torchmetrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f9150c-9e93-4012-8dc9-258d7efb4572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2497853934764862"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode='multiclass', num_classes=num_classes)\n",
    "smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\", zero_division=0).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378f72f-ff96-4a0c-b98f-43a90d9b56b2",
   "metadata": {},
   "source": [
    "This we why we recommend avoiding `mdmc_average` equal to samplewise, and calculating the metrics like for regular multiclass classifiers instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bac93-228b-4bb8-8a84-4d2ba48ff4cf",
   "metadata": {},
   "source": [
    "In conclusion when dealing with balanaced datasets, accuracy using the micro average plus `mdmc_average` global is sufficient,\n",
    "while the $F1$ score with the weighted average plus `mdmc_average` global is more accurate for imbalanaced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26bcacb",
   "metadata": {
    "id": "f26bcacb"
   },
   "source": [
    "{{ 'This fact is discussed further [here](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou)' | fndetail: 1 }}\n",
    "### References\n",
    "\n",
    "* [Torchmetrics Quickstart](https://torchmetrics.readthedocs.io/en/stable/pages/quickstart.html)\n",
    "* [Multiclass and multilabel classification in scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification)\n",
    "* [Segmentation Models Pytorch Metrics](https://smp.readthedocs.io/en/latest/metrics.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pet_breed_classification_post.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
