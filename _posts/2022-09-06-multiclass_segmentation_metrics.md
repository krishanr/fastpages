---
keywords: fastai
description: Multiclass segmentation metrics with torchmetrics, highlighting the difference between micro, macro, and macro-imagewise metrics.
title: Multiclass Segmentation Metrics
toc: true 
badges: true
comments: true
categories: [pytorch, torchmetrics, segmentationmodels]
nb_path: _notebooks/2022-09-06-multiclass_segmentation_metrics.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-09-06-multiclass_segmentation_metrics.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In my last <a href="{% post_url 2022-08-25-pet_segmentation %}">post</a> I showed how to use <code>torchmetrics</code> to implement segmentation metrics for the Oxford-IIIT pet segmentation dataset. We saw that in addition to the <code>average</code> keyword introduced in the <a href="{% post_url 2022-08-17-pet_breed_classification %}">pet breed classification</a> post, the <code>mdmc_average</code> keyword is necessary to compute metrics for image data.</p>
<p>In this post we'll dive deeper into these metrics, explaining the two choices for the <code>mdmc_average</code> parameter, including <em>global</em> and <em>samplewise</em>, as well as giving recommendations for dealing with imbalanced datasets.</p>
<p>The examples below will look primarily at precision and $F1$ score, but note that these metrics can be replaced by recall, dice score, etc.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install pytorch-lightning
<span class="o">!</span>pip install -U git+https://github.com/qubvel/segmentation_models.pytorch
<span class="o">!</span>pip install seaborn
<span class="o">!</span>pip install watermark
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">segmentation_models_pytorch</span> <span class="k">as</span> <span class="nn">smp</span>
<span class="kn">from</span> <span class="nn">torchmetrics.functional.classification</span> <span class="kn">import</span> <span class="n">precision</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">torchmetrics.classification</span> <span class="kn">import</span> <span class="n">StatScores</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Set the seed for reproduciblity.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To better understand the metrics, we'll work with a $4$ class problem with $n = 100$ samples. Classes $0$ and $3$ will have a probability of occurence of $\frac{1}{15}$, class $1$ will have a probability of $\frac{2}{3}$, and class $2$ will have a probability of $\frac{1}{5}$. We can generate data having this distribution using <code>torch.multinomial</code> below.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">)</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="p">:</span> <span class="n">x</span><span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">output</span><span class="p">[</span><span class="mi">70</span><span class="p">:,:,:,:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">target</span><span class="p">[</span><span class="mi">70</span><span class="p">:,:,:,:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For example, a subset of the output looks like:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:</span><span class="mi">10</span><span class="p">,:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[1, 1, 2, 1, 2, 2, 1, 1, 2, 2],
         [1, 1, 1, 1, 1, 3, 1, 1, 1, 1],
         [1, 1, 3, 2, 1, 2, 1, 1, 1, 1],
         [0, 1, 1, 2, 3, 1, 1, 1, 1, 2],
         [1, 0, 1, 1, 1, 1, 1, 1, 1, 3],
         [1, 1, 1, 2, 0, 1, 1, 0, 1, 1],
         [1, 1, 1, 0, 1, 1, 2, 1, 2, 1],
         [2, 1, 1, 1, 2, 1, 2, 1, 3, 2],
         [3, 1, 1, 3, 1, 2, 1, 1, 1, 1],
         [2, 3, 0, 1, 1, 1, 1, 2, 2, 1]]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we can collapse the image dimensions, $H$ and $W$, and then calculate metrics as for multiclass classification. This is precisely what happens
when we choose <code>mdmc_average</code> global.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">precision</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">,</span><span class="n">mdmc_average</span><span class="o">=</span><span class="s2">&quot;global&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.4517214596271515</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For comparisons sake, in scikit-learn we have:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.4517214441963613</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then the different options for <code>average</code> can be chosen, including <strong>micro</strong>, <strong>macro</strong>, and <strong>weighted</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In contrast, the image dimensions can be treated separately, which is called the <strong>macro-imagewise</strong> reduction:</p>
<ol>
<li>For each image and class the confusion table is computed over all pixels in an image.</li>
<li>Then the metric is computed for each image and class, as if it were a binary classifier.</li>
<li>The metrics are finally averaged over the images and classes.</li>
</ol>
<p>This is the most natural way to calculate metrics like the Jaccard index (intersection over union) for example. Unfortunately the jaccard index can't be calculated this way using <code>torchmetrics</code>. However the $F1$/Dice Score can be calculated using <code>torchmetrics</code>, and it's equivalent to the Jaccard index{% fn 1 %}:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">,</span><span class="n">mdmc_average</span><span class="o">=</span><span class="s2">&quot;samplewise&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.2497853934764862</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However if we calculate the $F1$ score using the segmentation models library, we get:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tn</span> <span class="o">=</span> <span class="n">smp</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">get_stats</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;multiclass&#39;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">smp</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;macro-imagewise&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.47478538751602173</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is because our dataset has many images with no targets (recall that we zeroed out several images). Thus the $F1$ score
for non-background classes reduces to $\frac{0}{0}$. <code>smp</code> replaces occurences of $\frac{0}{0}$ by $1$, while <code>torchmetrics</code> replaces $\frac{0}{0}$ by $0$.
If we pass <code>zero_division=0</code> to the segmentation models library, we get the same value as <code>torchmetrics</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tn</span> <span class="o">=</span> <span class="n">smp</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">get_stats</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;multiclass&#39;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">smp</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;macro-imagewise&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.2497853934764862</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This we why we recommend avoiding <code>mdmc_average</code> equal to samplewise, and calculating the metrics like for regular multiclass classifiers instead.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In conclusion when dealing with balanaced datasets, accuracy using the micro average plus <code>mdmc_average</code> global is sufficient,
while the $F1$ score with the weighted average plus <code>mdmc_average</code> global is more accurate for imbalanaced datasets.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> --iversions
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>seaborn                    : 0.11.2
matplotlib                 : 3.5.1
segmentation_models_pytorch: 0.3.0
torch                      : 1.12.0
sklearn                    : 1.0.2

</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{{ 'This fact is discussed further <a href="https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou">here</a>' | fndetail: 1 }}</p>
<h3 id="References">References<a class="anchor-link" href="#References"> </a></h3><ul>
<li><a href="https://torchmetrics.readthedocs.io/en/stable/pages/quickstart.html">Torchmetrics Quickstart</a></li>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification">Multiclass and multilabel classification in scikit-learn</a></li>
<li><a href="https://smp.readthedocs.io/en/latest/metrics.html">Segmentation Models Pytorch Metrics</a></li>
</ul>

</div>
</div>
</div>
</div>
 

