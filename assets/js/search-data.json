{
  
    
        "post0": {
            "title": "How to build a Docker image with an Anaconda environment in Paperspace?",
            "content": "Paperspace is an affordable tool for obtaining cloud compute power. Using it, you can run jupyter notebooks, python scripts, or just about anything, provided you can write the right Docker file. Plus some of the compute instances also come with GPUs, making Paperspace an alternative to Google Colab. In contrast with Google Colab though, Paperspace saves the files you use, so they are there when you return. Inspired by Alex&#39;s post on Building my own image to use IceVision with Paperspace, in this post I&#39;ll describe how you can build a docker image to wrap an Anaconda environment, and then run it on Paperspace. The Dockerfile and Anaconda environment I used are available on Github here. . The context for this post is that even though a custom anaconda environment can be created and run in the Paperspace images that are readily available, for longer projects, it would be useful to use a Paperspace image that comes prebuilt with a desired environment. . We can do this by adding a few additional lines to the standard Paperspace Docker image. Any anconda environment available at &#39;environment.yml&#39; can be created and activated using these 4 lines: . ADD environment.yml $APP_HOME/environment.yml RUN conda env create -f $APP_HOME/environment.yml ENV PATH $CONDA_DIR/envs/tf2/bin:$PATH RUN echo &quot;source activate tf2&quot; &gt; ~/.bashrc . The first two commands add the conda environment file to the Docker containers workspace, and then create the conda environment. . The third command adds the enviornment directory to the PATH enviroment variable. The directory path contains the environment name, which in this case is tf2 (replace this with your environment name if necessary). Finally, the last command adds a command to activate the enviornment to the bashrc file, so that the environment is activated when the container is started. . That&#39;s it! The bulk of the work for creating and running a Docker image with an anaconda environment in Paperspace is done with the above 4 lines of code. . The entire Dockerfile is available on GitHub here. . Following Alex, I&#39;ll describe the steps to setup a Paperspace instance using this Dockerfile. . First go to your notebooks inside a project, and create a new Paperspace notebook. . . Next, choose a GPU instance to create the notebook (or CPU depending on the project), for example the RTX5000 option, and an automatic shutdown of 6 hours. . . Next the Dockerfile will have to be built and pushed to Dockerhub, before it can be used with paperspace. . To build the Dockerfile, just run . docker build -t k2rajara/paperspace:3.0 . for example, in the same directory as the Dockerfile. . In the above code, &#39;k2rajara/paperspace&#39; is the container name on Dockerhub. To create your own repository on Dockerhub, follow the instructions here. Then push your image there. Once this is complete, its details can be added to the advanced section: . . The command that is run in the container is the default one for Paperspace: . jupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin=&#39;*&#39; --ServerApp.allow_credentials=True . Finally after clicking &quot;Start Notebook&quot;, Paperspace should pull the image from Dockerhub and run jupyter lab as requested. . In my case, the conda environment I created is available on Github here and shown here: . name: tf2 channels: - conda-forge - defaults - anaconda dependencies: - python=3.6 - pip=21 - jupyterlab - jupyter_client=7.1.0 - jupyter_server=1.13.1 - tensorflow-gpu=2.2 - pip: - mlflow==1.11.0 - python-dotenv==0.15.0 - matplotlib==3.1.2 - Pillow==8.0.1 - scikit-image==0.17.2 - ruamel_yaml==0.16.12 . The project I was working on used python 3.6, and tensorflow 2.2. Due to the outated version of python, I also had to install the jupyter client and server manually, to avoid bugs. However, the environment can be customized depending on your project requirements. . For example, to create a minimal installation with tensorflow2 and jupyterlab, use: . name: tf2 channels: - anaconda dependencies: - jupyterlab - tensorflow-gpu .",
            "url": "https://krishanr.github.io/fastpages/docker/tools/2022/06/15/anaconda-in-docker-on-paperspace.html",
            "relUrl": "/docker/tools/2022/06/15/anaconda-in-docker-on-paperspace.html",
            "date": " • Jun 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Principal component analysis",
            "content": "Principal component analysis (PCA) is a well known technique for dimensionality reduction, dating back over one hundred years. Here we&#39;ll summarize the theory and application of PCA, showing for example that it&#39;s most naturally applicable to multivariate normal data, which is determined by its first two moments. We shall derive PCA as a canonical form for random vectors, obtained using the population covariance matrix for a population or the sample covariance matrix for a sample. We&#39;ll also see how this decomposition differs when using the population correlation matrix or the sample correlation matrix instead. We conclude with a computation of PCA for the iris dataset, first from scratch using numpy, and then using the scikit-learn API. . We follow chapter 8 in (Johnson &amp; Wichern, 1982) for the theory of PCA, both in terms of populations of random variables and samples of random variables, and then apply it following chapter 8 in (Géron, 2017). We also follow Professor Helwig&#39;s notes available here. . By making a distinction between population and sample PCA, we can make statements about limits of the sample PCA as the sample size goes to infinity. Besides theoretical interest, these limits are useful for calculating confidence intervals, p-values, etc. . Note hereafter that all vectors are assumed to row vectors unless specified otherwise, and we&#39;ll use $ vec{1}$ to denote the row vector with components all equal to $1$. We&#39;ll also make the following definitions. Let $ vec{X} = (X_1, ..., X_p)$ be a random (row) vector. The vector has (population) mean $ vec{ mu} = mathbb{E}[ vec{X}]$ and (population) covariance matrix $ Sigma = mathbb{E}[( vec{X} - vec{ mu})^T( vec{X} - vec{ mu})] $. The (population) correlation matrix is . $$ P = begin{bmatrix} 1 &amp; rho_{12} &amp; rho_{13} &amp; dots &amp; rho_{1p} rho_{21} &amp; 1 &amp; rho_{23} &amp; dots &amp; rho_{2p} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots rho_{p1} &amp; rho_{p2} &amp; rho_{p3} &amp; dots &amp; 1 end{bmatrix} $$where . $$ begin{aligned} rho_{jk} &amp; = frac{ sigma_{jk}}{ sqrt{ sigma_{jj} sigma_{kk} }}, &amp; sigma_{jk} &amp; = Sigma_{jk} end{aligned} $$is the Pearson correlation coefficient between variables $X_j$ and $X_k$. . Transformations of random vectors . For $ vec{v}, vec{u} in mathbb{R}^p$, let $Y = vec{X} vec{v}^T = vec{X} cdot vec{v}$ and $Z = vec{X} cdot vec{u}$, then one can show that . $$ begin{aligned} mathbb{E}[Y] &amp; = vec{ mu} cdot vec{v} operatorname{cov}[Y, Z] &amp; = vec{v} : Sigma : vec{u}^T end{aligned} $$More generally, with $v_1, dotsc,v_q in mathbb{R}^p$ and $c in mathbb{R}^q$, let $Y = vec{X} cdot vec{v_i}$, and define the $q times p$ matrix $V$ by: . $$ V = begin{bmatrix} vec{v}_1 vdots vec{v}_q end{bmatrix} $$so that $ vec{Y} = vec{X} cdot V^T$, then . $$ begin{aligned} mathbb{E}[ vec{Y} + c] &amp; = vec{ mu} cdot V^T + c operatorname{cov}[ vec{Y} + c ] &amp; = operatorname{cov}[ vec{X} cdot V^T] = V ; Sigma_X ; V^T end{aligned} $$Also note that . $$ sum_{j=1}^q operatorname{Var}(Y_j) = operatorname{tr}( operatorname{cov}[ vec{Y}]) = operatorname{tr}(V ; Sigma_X ; V^T) = operatorname{tr}( Sigma_X ; V^T V) $$ . by the cyclic property of the trace. Hence when $q = p$ and $V$ is orthonormal, . $$ sum_{j=1}^p operatorname{Var}(Y_j) = operatorname{tr}( operatorname{cov}[ vec{Y}]) = operatorname{tr}(A ; Sigma_X ; A^T) = operatorname{tr}( Sigma_X ) = sum_{j=1}^p operatorname{Var}(X_j) $$ . Spectral theorem for symmetric matricies . Recall for any $p times p$ symmetric matrix $ Sigma$, if we let $ lambda_1 geq lambda_2 geq dots geq lambda_p geq 0$ be the eigenvalues of $ Sigma$, then . $$ begin{aligned} lambda_1 &amp; = operatorname{max}_{x neq 0} frac{ vec{x} : Sigma : vec{x}^T}{ lVert x rVert^2} lambda_p &amp; = operatorname{min}_{x neq 0} frac{ vec{x} : Sigma : vec{x}^T}{ lVert x rVert^2} frac{ lambda_1}{ lambda_p} &amp; = kappa( Sigma) = operatorname{max}_{x , y neq 0} frac{ vec{x} : Sigma : vec{x}^T}{ lVert x rVert^2} frac{ lVert x rVert^2}{ vec{y} : Sigma : vec{y}^T} end{aligned} $$where $ kappa( Sigma)$ is the condition number of $ Sigma$, defined when $ Sigma$ is positive definite. There are also similar formulas for the other eigenvalues defined on appropriate subspaces. . Finally, since the covariance matrix $ Sigma$ is positive semi-definite, the spectral theorem applies to it, which we shall review later. This together with the fact that any random variable with zero variance is zero implies that $ Sigma$ is positive definite iff the random variables $X_1, ..., X_p$ are linearly independent. . Two dimensions . Here&#39;s a useful corollary of the above transformation law for the covariance matrix. Suppose $X_1$ and $X_2$ are random variables with covariance matrix $ Sigma_X$. Then the variables $Y_1$ and $Y_2$ defined by . $$ begin{aligned} Y_1 &amp; = X_1 - X_2 Y_2 &amp; = X_1 + X_2 end{aligned} $$have covariance . $$ Sigma_Y = operatorname{cov}[ vec{Y}] = begin{bmatrix} sigma_{11} - 2 sigma_{12} + sigma_{22} &amp; sigma_{11} - sigma_{22} sigma_{11} - sigma_{22} &amp; sigma_{11} + 2 sigma_{12} + sigma_{22} end{bmatrix} $$Thus $Y_1$ and $Y_2$ are uncorrelated iff $X_1$ and $X_2$ have the same variance, i.e. $ sigma_{11} = sigma_{22}$. . Moreover if $X_1$ and $X_2$ are independent, then $Y_1$ and $Y_2$ are in general not independent. In fact, if $Y_1$ and $Y_2$ are also independent, then $X_1$ and $X_2$ are normal random variables (see here). This fact is known as Bernstein&#39;s theorem, and holds for more general linear combinations. . Thus a unitary transformation of iid random variables have the same covariance matrix but are not in general independent. . Population PCA . As above let $ vec{Y} = vec{X} cdot V^T$ where $V$ is a $pxp$ matrix. Then the formula . $$ Sigma_Y = operatorname{cov}[ vec{Y}] = V ; Sigma_X ; V^T $$ . shows that $ Sigma_Y$ and $ Sigma_X$ are similar quadratic forms. Thus since $ Sigma_X$ is symmetric, by the spectral theorem there exists a new set of random variables $Y_1, dotsc, Y_p$ in which $ Sigma_Y$ is diagonal. These are the principal components of the random vector $ vec{X}$, which can also be defined using notations from probability theory as follows: . $$ begin{aligned} text{First principal component } = Y_1 = &amp; text{ linear combination of } vec{X} vec{v}_1^T text{ that maximizes } &amp; operatorname{Var}[ vec{X} vec{v}_1^T] text{ subject to } lVert vec{v}_1 rVert = 1 text{Second principal component } = Y_2 = &amp; text{ linear combination of } vec{X} vec{v}_2^T text{ that maximizes } &amp; operatorname{Var}[ vec{X} vec{v}_2^T] text{ subject to } lVert vec{v}_2 rVert = 1 text{ and } vec{v_2} : Sigma : vec{v_1}^T = operatorname{Cov}[ vec{X} vec{v}_1^T, vec{X} vec{v}_2^T] = 0 &amp; vdots text{Last principal component } = Y_p = &amp; text{ linear combination of } vec{X} vec{v}_p^T text{ that maximizes } &amp; operatorname{Var}[ vec{X} vec{v}_p^T] text{ subject to } lVert vec{v}_p rVert = 1 text{ and } operatorname{Cov}[ vec{X} vec{v}_p^T, vec{X} vec{v}_1^T] = operatorname{Cov}[ vec{X} vec{v}_p^T, vec{X} vec{v}_2^T] = dots = operatorname{Cov}[ vec{X} vec{v}_p^T, vec{X} vec{v}_{p-1}^T] = 0 end{aligned} $$Equivalently, let $( lambda_1, vec{e}_1)$, $( lambda_2, vec{e}_2)$, ..., $( lambda_p, vec{e}_p)$ be the eigenvalue-eigenvector pairs of $ Sigma$ with the eigenvalues ordered in non-increasing order. Then the ith principal component is: . $$ Y_i = vec{X} vec{e}_i^T quad, i = 1,...,p $$with . $$ begin{aligned} operatorname{Var}[Y_i] &amp; = lambda_i quad i = 1,...,p operatorname{Cov}[Y_i, Y_j] &amp; = 0 quad i neq j end{aligned} $$Thus the ith maximal variance of $ Sigma$ is the ith eigenvalue of $ Sigma$. . Also note that only unique eigenvalues have unique eigenvectors (up to sign), so the principal components are in general not unique. . The above formula for the trace implies that the random variables $Y_1, ..., Y_p$ have the same total variance as $X_1, ..., X_p$, since . $$ sum_{j=1}^p operatorname{Var}(X_j) = operatorname{tr}( Sigma) = operatorname{tr}(V ; Sigma_X ; V^T ) = operatorname{tr}( Sigma_X) = sum_{j=1}^p operatorname{Var}(Y_j) = sum_{j=1}^p lambda_j $$ . Thus we can define the proportion of variance captured by the $k$th principal component as . $$ begin{aligned} &amp; text{Proporition of variance due } = frac{ lambda_k}{ sum_{j=1}^p lambda_j}, quad k= 1,...,p &amp; text{to the $k$th principal component } end{aligned} $$The basic idea behind PCA for dimensionality reduction is to only keep the the first $k$ principal components which captures, say $90 %$, of the variance, thereby reducing the number of variables. . Another way to select the principal components is to choose the most variables possible such that the conditon number (ratio of largest to smallest eigenvalues) of the reduced covariance matrix is close to $1$. This is because machine learning algorithms, like linear and logistic regression, preform better (more precision and faster convergence for gradient based methods) when the condition number of the covariance matrix is close to $1$. . Population PCA using the correlation matrix . PCA can also be calculated using the correlation matrix $P$ which is the covariance matrix of the standardized features having their mean set to $0$ and variance to $1$. Specifically, one applies the above calculations to the following variables: . $$ begin{aligned} Z_1 &amp;= frac{X_1 - mu_1}{ sqrt{ sigma_{11}}} Z_2 &amp;= frac{X_2 - mu_2}{ sqrt{ sigma_{22}}} vdots &amp; quad vdots Z_p &amp;= frac{X_p - mu_p}{ sqrt{ sigma_{pp}}} end{aligned} $$In matrix notation we have . $$ begin{aligned} vec{Z} &amp;= ( vec{W}^{ frac{1}{2}})^{-1}( vec{X} - vec{ mu}) operatorname{cov}[ vec{Z}] &amp; = ( vec{W}^{ frac{1}{2}})^{-1} ; Sigma ; ( vec{W}^{ frac{1}{2}})^{-1} = P end{aligned} $$where $ vec{W} = operatorname{diag}( sigma_{11},..., sigma_{pp})$. Then one obtains a decomposition similar to above with the following difference in the formula: . $$ sum_{j=1}^p operatorname{Var}(Y_j) = sum_{j=1}^p operatorname{Var}(X_j) = p $$Then the formula for the proportion of variance captured by the $k$th principal component simplifies to: . $$ begin{aligned} &amp; text{Proporition of variance due } = frac{ lambda_k}{p}, quad k= 1,...,p &amp; text{to the $k$th principal component } end{aligned} $$It&#39;s important to note that the principal components calculated from $ Sigma$ and $P$ can differ significantly, being a different linear combination of the original variables (Example 8.2 in (Johnson &amp; Wichern, 1982)). The principal components are similar when the variances for the $p$ random variables are the same, $ sigma = sigma_{11} = sigma_{22} = dots = sigma_{pp}$. Finally note that it&#39;s good practise to work with variables on the same scales so that the principal components aren&#39;t heavily skewed towards the larger variables. . Sample PCA . Consider $n$ iid realizations $ vec{X}_1, ..., vec{X}_n$ from the random vector $ vec{X}$. We can arrange this data into a feature matrix . $$ X = begin{bmatrix} vec{X}_1 vdots vec{X}_n end{bmatrix} $$The sample covariance matrix, $S$, is: . $$ begin{aligned} S_{i j} = frac{1}{n-1} sum_{k=1}^n (X_{k i} - bar{X}_i) (X_{k j} - bar{X}_j) = frac{1}{n-1} (X - vec{1} otimes bar{X})^T (X - bar{X} otimes vec{1}) end{aligned} quad quad i= 1,...,p text{ and } j = 1,...,p$$and the sample correlation matrix, $R$, is: . $$ begin{aligned} R_{ij} = frac{S_{ij}}{ sqrt{S_{ii}} sqrt{S_{jj}}} = frac{ sum_{k=1}^n (X_{k i} - bar{X}_i) (X_{k j} - bar{X}_j)}{ sqrt{ sum_{k=1}^n (X_{k i} - bar{X}_i)^2 } sqrt{ sum_{k=1}^n (X_{k j} - bar{X}_j)^2}} end{aligned} quad quad i= 1,...,p text{ and } j = 1,...,p $$For $ vec{v} in mathbb{R}^p$ a new feature is defined by the equation . $$ Y = vec{X} cdot vec{v} $$ . Formulas similar to above show that the sample covariance matrix transforms according to (see chapter 8 in (Johnson &amp; Wichern, 1982)): . $$ S_Y = V S_X V^T $$ . Thus an identical construction can be given for the principal components, now using $S$ in place of $ Sigma$, or $R$ in place of $P$. . Reconstruction error . Let $W_d = [ v_1 dots v_d ]$ be the matrix whose columns are the first $d$ eigenvectors of $S_X$ (ordered from largest to smallest eigenvalue). Then given any feature vector $ vec{X}_i$, the vector $ vec{X}_i W_d$ is the $d$ dimensional projection, and the vector $ vec{X}_i W_d W_d^T$ is the reconstructed vector. There&#39;s a fairly simple formula for the reconstruction error . $$ sum_{i=1}^n || vec{X}_i W_d W_d^T - vec{X}_i||^2 $$It turns out that (see chapter 9 in (Johnson &amp; Wichern, 1982) for a proof) . $$ begin{aligned} sum_{i=1}^n || vec{X}_i W_d W_d^T - vec{X}_i||^2 &amp; = operatorname{tr}[(X W_d W_d^T - X)(X W_d W_d^T - X)^T] &amp; vdots &amp; = lambda_{d+1} + dots + lambda_p end{aligned} $$Thus the recontruction error of the $d$ dimensional projection is equal to the remaining $p-d$ eigenvalues of the covariance matrix, i.e. the unexplained variance. Finally, note that this is the smallest possible error among all $d$ dimensional projections, and this is another defining property of PCA. . Large sample properties . As expected, when the feature matrix $X$ comes from a multivariate Gaussian, the principal components recover the natural frame which diagonalizes the covariance matrix of the Gaussian. In fact, much more can be said. . When $ vec{X}_i overset{iid}{ sim} N( vec{ mu}, vec{ Sigma})$ and the eigenvalues of $ Sigma$ are strictly positive and unique: $ lambda_1 &gt; dots lambda_p &gt; 0$, we can describe the large sample properties of the principal directions and eigenvalues. Then as $n rightarrow infty$, we have ((Johnson &amp; Wichern, 1982)): . $$ sqrt{n}( hat{ vec{ lambda}} - vec{ lambda}) approx N( vec{0}, 2 vec{ Lambda}^2) sqrt{n}( hat{ vec{v}}_k - vec{v}_k) approx N( vec{0}, vec{V}_k) $$where $ Lambda = operatorname{diag}( lambda_1, ..., lambda_k)$ and . $$ vec{V}_k = lambda_k sum_{l neq k} frac{ lambda_l}{( lambda_l - lambda_k)^2} vec{v}_l^T vec{v}_l $$Furthermore, as $n rightarrow infty$, we have that $ hat{ lambda}_k$ and $ hat{ vec{v}}_k$ are independent. . Practical considerations . Below we follow A. Geron&#39;s notes on PCA to implement it in numpy and sckit-learn for the iris dataset. . import pandas as pd from sklearn import datasets from sklearn.decomposition import PCA import numpy as np iris = datasets.load_iris() . print(iris[&#39;feature_names&#39;]) . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . df = pd.DataFrame(iris[&#39;data&#39;], columns=[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]) df.head(5) . sepal_length sepal_width petal_length petal_width . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . df.describe() . sepal_length sepal_width petal_length petal_width . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | . Since the units of each feature are comparable (the standard deviations are of the same order of magnitude), we will calculate PCA using the sample covariance matrix. If on the otherhand, the magnitudes differed by orders of magnitude, we would calculate PCA using the sample correlation matrix. . PCA using SVD directly . The principal components can be calculated by diagonalizing the covariance matrix or calulating the singular value decomposition of the feature matrix. We&#39;ll use the latter for numerical reasons. . X = iris[&#39;data&#39;] . The SVD of $X$ is calculated after subtracting off the sample mean. We&#39;ll call this matrix $X_{centered}$ and work with it hereafter. Then the SVD of $X_{centered}$ is . $X_{centered} = U D V^T$ . where $U$ is an $n times n$ orthonormal matrix, $V$ is an $m times m$ orthonormal matrix, and $D = operatorname{diag}(d_{11}, ..., d_{mm})$ is an $n times m$ rectangular diagonal matrix. . The matrix $V$ is the matrix of eigenvectors from the sample PCA above, and the diagonal covariance matrix $S_Y$ is obtained by squaring the singular values and dividing by $n-1$: . $$ S_Y = frac{1}{n-1} begin{bmatrix} d_{11}^2 &amp; &amp; &amp; ddots &amp; &amp; &amp; d_{mm}^2 end{bmatrix} $$For this example, we have: . X_centered = X - X.mean(axis=0) U, D, Vt = np.linalg.svd(X_centered) V = Vt.T eigs = D ** 2 . The $d$-dimensional projection can be calcualted using the $p times d$ matrix $W_d$ whose $d$ columns are the first $d$ eigenvectors. . $X_{d-proj} = X_{centered} W_d$ . W2 = V[:, :2] # W2 can also be obtained from the sklearn package using: pca.components_.T X2D_diy = X_centered @ W2 X2D_diy.shape . (150, 2) . Obtain the reconstructed data using: . $ X_{recovered} = X_{d-proj} W_{d}^T$ . A defining property of PCA is that for any dimension $d &lt; m$, the PCA reconstruction of the data minimizes the mean squared error between the original data among all possible $d$ dimensional hyperplanes. Moreover, the square of this error is equal to the sum of the $m-d$ smaller eigenvalues, as we shall see. . X_recovered = X2D_diy @ W2.T . The squared error is then: . np.linalg.norm(X_centered - X_recovered)**2 . 15.204644359438948 . Which is equal to the sum of the two smaller eigenvalues: . sum(eigs[2:]) . 15.204644359438953 . The proportion of the total sample variance due to each sample principal component is: . $$ frac{ hat{ lambda_i}}{( sum_{i=1}^m hat{ lambda_i}) } quad quad quad i = 1,...,m $$and is given numerically by . eigs/sum(eigs) . array([0.92461872, 0.05306648, 0.01710261, 0.00521218]) . We can obtain the number of principal components to retain by either (i) retaining the first $d$ components which sum up to 95% of the total variance, or (ii) looking for an elbow in the scree plot. . import matplotlib.pyplot as plt plt.plot(range(1,len(eigs)+1), np.array(eigs.tolist())) plt.title(label=&quot;Scree plot&quot;) plt.xlabel(&quot;i&quot;) plt.ylabel(&quot;lambda_i&quot;) plt.show() . There is an elbow in the plot at $i = 2$, meaning all eigenvalues after $ lambda_1$ are relatively small and about the same size. In this case, it appears that the first principal component effectively summarize the total sample variance. . Finally, we can visualize the two dimensional projection using a scatter plot. . plt.title(&quot;PCA with 2 principal components&quot;, fontsize=14) plt.scatter(X2D_diy[:, 0], X2D_diy[:,1], c=iris[&#39;target&#39;]) plt.xlabel(&quot;$y_1$&quot;, fontsize=18) plt.ylabel(&quot;$y_2$&quot;, fontsize=18) plt.show() . PCA using scikit-learn . Now we&#39;ll calculate the PCA projection using scikit-learn. Notice that scikit-learn will mean center the data itself. . pca = PCA(n_components = 2) X2D = pca.fit_transform(X) . Instead of specifing the number of components in the call to PCA above, we can also set $ operatorname{n _components}=0.95$ to obtain the number of dimensions which capture 95% of the variance in the data. Or we could pass in no arguments to get the full SVD and plot a scree plot. . Below, we check that sklearn PCA and the one calculated with SVD directly give the same answer. . np.max(X2D - X2D_diy) . 2.748330173586095 . The non-uniquness of the PCA leads to a large difference. However, this can be fixed by inspecting the projections and adjusting: . X2D_diy[:,1] = - X2D_diy[:,1] np.max(X2D - X2D_diy) . 1.3322676295501878e-15 . The explained variance ratio holds the proportion of variance explained by the first two sample principal components, and is given by . pca.explained_variance_ratio_ . array([0.92461872, 0.05306648]) . Computational complexity . Let $n_{max} = operatorname{max}(m,n)$ and $n_{min} = operatorname{min}(m,n)$. If $n_{max} &lt; 500$ and $d$ is less than $80 %$ of $n_{min}$, then scikit-learn uses the full SVD approach which has a complexity of $O(n_{max}^2 n_{min})$, otherwise it uses randomized PCA which has a complexity of $O(n_{max}^2 d)$ (cf. here and (Géron, 2017)). . There are many more variations of PCA, and other dimensionality reduction algorithms. See Decomposing signals in components (matrix factorization problems) in the scikit-learn documentation, for example. . Johnson, R. A., &amp; Wichern, D. W. (1982). Applied Multivariate Statistical Analysis. | Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. | .",
            "url": "https://krishanr.github.io/fastpages/statistics/2022/03/30/principal-component-analysis.html",
            "relUrl": "/statistics/2022/03/30/principal-component-analysis.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Standard probability distributions",
            "content": "We&#39;ll review elementary univariate probability distributions in statistics, including how the mean/variance are derived, simple relationships, and the questions they answer, with calculations done in scipy. . This review is similar in spirit to brand name distributions and univariate distribution relationships. . import scipy.special as sp import scipy.stats as ss import numpy as np import matplotlib.pyplot as plt import seaborn as sns . The univariate probability distributions answer questions about events occuring on a discrete time interval $D_n = [0, 1, dotsc, n ]$ or a continuous time interval $I_t = [0, t]$. . In the discrete case each event could be a success with probability $p$ or a failure with probability $1-p$, with different events being independent. Then we can ask the following questions: . For fixed $n$, what is the probability of observing $k$ successes? | How many events do we have to observe before seeing $1$ success or more generally, $r$ successes? | The first question is answered by the Binomial distribution, and the second is answered by the geometric and negative binomial distributions. A concrete example of such events are coin tosses, where the outcome of each toss is independent of previous tosses, and each toss comes up as heads with probability $p = frac{1}{2}$. . In the case of a continuous time interval $I_t$ there are many types of events that can occur, but we&#39;ll study the simplest type with real world applications, the events associated with a Poisson process. Then we can ask the following questions about a Poisson process: . Within a fixed time $t$, what is the probability of observing $k$ events? | How much time do we have to wait before seeing the first event or more generally, $r$ events? | The first question is answered by the Poisson distribution, and the second is answered by the exponential and Erlang (Gamma) distributions. A concrete example of events following a Poisson process are the distribution of earthquakes in a given region. . Finally, the normal distribution will arise as the limiting distribution in the famous central limit theorem. . Moving forward, do remember that . $$ operatorname{var}[X] = mathbb{E}[(X - mathbb{E}[X] )^2] = mathbb{E}[X^2] - ( mathbb{E}[X])^2 $$ . Bernoulli distribution . Suppose X is a random variable denoting having a trial with a probability of success $p$. It gives a success, $X = 1$, with probability $p$ and failure, $X = 0$, with probability $1-p$. Then $X sim operatorname{Ber}(p)$, and . $$ mathbb{E}[X] = p, : : mathbb{E}[(X - mathbb{E}[X] )^2] = p (1-p)$$ . p = 0.4 rv = ss.bernoulli(p) mean, var, skew, kurt = rv.stats(moments=&#39;mvsk&#39;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 0.4, var 0.24, skew 0.40824829046386296, kurt -1.8333333333333337 . rv.interval(0.99) . (0.0, 1.0) . fig, ax = plt.subplots() y = np.random.binomial(1, p, size=100) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(loc=&#39;upper right&#39;) ax.set(title=&#39;Bernoulli Distribution&#39;, xlabel=&#39;y&#39;); . P(X = 1) is calculated using the probability mass function below: . rv.pmf(1) . 0.4 . Binomial distribution . When we have $n$ independent trials, each with a probability of success $p$ and failure $1-p$, then $X sim operatorname{Bin}(n, p)$ which is a sum of $n$ independent $ operatorname{Ber}(p)$ random variables. I.e. letting $X_i sim operatorname{Ber}(p)$ for $i = 1, dotsc, n$, then . $$ X = sum_{i=1}^n X_i $$ . and a straightforward calculation gives . $$ mathbb{E}[X] = n p, : : mathbb{E}[(X - mathbb{E}[X] )^2] = n p (1-p)$$ . p, n = 0.4, 10 rv = ss.binom(n, p) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 4.0, var 2.4, skew 0.12909944487358052, kurt -0.1833333333333334 . fig, ax = plt.subplots() y = np.random.binomial(n, p, size=200) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Binomial distribution&quot;, xlabel=&quot;y&quot;); . P(X = 3) is calculated using the probability mass function below: . rv.pmf(3) . 0.21499084799999976 . Do a quick check that the probability mass function is normalized: . sum (rv.pmf(i) for i in range(n+1)) . 0.9999999999999994 . Geometric distribution . Suppose independent trials, each with a probability of success $p$, are performed until success occurs. Then the number of trials until the first success occurs is a random variable $X sim operatorname{Geo}(p)$, and called a geometric random variable. Its probability mass function is: . $$ P(X= n) = (1-p)^{n-1} p, : : n= 1,2, dotsc $$ . Using the probability generating function we calculate: . $$ mathbb{E}[X] = frac{1}{p}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = frac{1-p}{p^2}$$ . p = 0.4 rv = ss.geom(p) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 2.5, var 3.749999999999999, skew 2.0655911179772892, kurt 6.2666666666666675 . fig, ax = plt.subplots() y = np.random.geometric(p, size=1000) sns.countplot(x=y, label =f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Geometric Distribution&quot;, xlabel=&quot;y&quot;); . As the above histogram hints at, the geometric distribution is the discretization of the exponential distribution, and shares the memoryless property: . $$ P(X &gt; n + k | X &gt; k) = P(X &gt; n) $$ . Negative Binomial distribution . Suppose independent trials, each with a probability of success $p$, are performed until success occurs (with the last trial being a success). Then the number of trials until $r$ successes occur is a random variable $X sim operatorname{NegBin}(r,p)$ and called a negative binomial random variable, which can be expressed as a sum of $r$ independent geometric distributions $X_i sim operatorname{Geo}(p)$ for $i = 1, dotsc, n$. . $$ X = X_1 + X_2 + dotsc + X_n $$ . The probability mass function is: . $$ P(X = k) = { k + r - 1 choose k} p^r (1-p)^{k} $$ . where $k$ is the number of failures seen before seeing a total of $r$ successes. . $$ mathbb{E}[X] = frac{r (1-p)}{p}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = r frac{(1-p)}{p^2}$$ . p = 0.4 r = 4 rv = ss.nbinom(r, p) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 5.999999999999999, var 14.999999999999996, skew 1.0327955589886446, kurt 1.5666666666666669 . fig, ax = plt.subplots() y = np.random.negative_binomial(r, p, size=1000) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Negative Binomial distribution&quot;, xlabel=&quot;y&quot;); . Poisson distribution and processes . The number of events occuring in a fixed time interval $t$, with each event occuring independently of each other and at a average constant rate $ lambda$ is described by the Poisson distribution with parameter $ mu = lambda t$, $X sim Poi( mu)$, and has the probability mass function: . $$ P(X = k) = frac{ mu^k}{k!} e^{- mu} : : k = 0,1, dotsc$$ . The standard moments are, . $$ mathbb{E}[X] = mu = lambda t, : : mathbb{E}[(X - mathbb{E}[X] )^2] = mu$$ . Note that the time till the first event in a Poisson process is described by the exponential distribution with parameter $ lambda$, and the time till the first $r$ events in a Poisson process is described by the Erlang (Gamma) distribution with shape $r$ and rate $ lambda$. See below for definitions. . Also see section 20.2 from the stats cookbook for a good summary of the Poisson process and the different distributions involved. . mu = 6 rv = ss.poisson(mu) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 6.0, var 6.0, skew 0.408248290463863, kurt 0.16666666666666666 . fig, ax = plt.subplots() y = np.random.poisson(mu, size=1000) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Poisson distribution&quot;, xlabel=&quot;y&quot;); . Gamma function and distribution . Recall the Gamma function, $ Gamma( alpha)$, is a function of a real variable, such that $ Gamma( alpha) = alpha!$ for $ alpha = 0, 1, 2 dotsc$. . The Gamma distribution then has the probability distribution: . $$f(x; alpha, beta) = frac{x^{ alpha-1} e^{- beta x} beta^ alpha}{ Gamma( alpha)} text{ for } x &gt; 0$$ . with $ alpha &gt;0$ called the shape and $ beta &gt;0 $ called the rate. $ theta = 1/ beta$ is also called the scale parameter. . If $X sim operatorname{Ga}( alpha, beta)$ (see (Murphy, 2012)), then the standard moments are . $$ mathbb{E}[X] = frac{ alpha}{ beta}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = frac{ alpha}{ beta^2}$$ $$ operatorname{mode}{X} = frac{( alpha-1)}{ beta} $$ . We parameterize the Gamma distribution in scipy as follows. . alpha, beta = 3, 1 rv = ss.gamma(a=alpha,scale=1/beta) . rv.interval(0.99) . (0.33786338872773347, 9.273792089255544) . fig, ax = plt.subplots(1, 1) x = np.linspace(0, rv.ppf(0.999), 100) ax.plot(x, rv.pdf(x), &#39;r-&#39;, lw=2, alpha=0.6, label=&#39;gamma pdf&#39;) ax.set(title=&quot;Gamma PDF&quot;, xlabel=&quot;y&quot;); . Exponential distribution . The Exponential, Erlang, and Chi-Squared distributions are all related to the Gamma distribution (Murphy, 2012), but we&#39;ll describe the simplest, the exponential distribution. . $$ operatorname{Exp}(x | lambda) = operatorname{Ga}(x | 1, lambda)$$ . where $ lambda$ is the rate parameter. The exponential distribution describes the waiting time between events in a Poisson process. . From the above formulas, if $X sim operatorname{Exp}(x | lambda)$, then . $$ mathbb{E}[X] = frac{1}{ lambda}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = frac{1}{ lambda^2}$$ $$ operatorname{mode}{X} = 0 $$ . The exponential distribution, which is the continuous analog of the geometric distribution, also has the memoryless property: . $$ P(X &gt; t + h | X &gt; h) = P(X &gt; t )$$ . for any $t, h &gt; 0 $. . Characteristic functions of random variables . The charactierstic function will be useful for working with these distributions. . The characteristic function (also called the Fourier transform) of a random variable $X$ is . $$ phi(t) = mathbb{E}[e^{i t X}] : : text{ for } t in mathbb{R}$$ . This function satisfies $| phi(t)| &lt;= 1$ for all $t$ and is uniformly continuous. It has some advantages over the moment generating function, including the fact that it&#39;s defined everywhere, and that it uniquely determines the random variable (Grimmett &amp; Stirzaker, 2001). . Some other facts that we can derive from characteristic functions: . If $ phi^k(0)$ and $ mathbb{E}[|X^k|] &lt; infty$, then | $$ phi^k(0) = i^k mathbb{E}[X^k] $$ . If $X_1, X_2, dotsc, X_n$ are independent then | $$ phi_{X_1 + dotsc + X_n}(t) = phi_{X_1}(t) dots phi_{X_n}(t) $$ . If $a, b in mathbb{R}$ and $Y = a X + b $ then | $$ phi_{Y}(t) = e^{i t b } phi_{a X}(t) $$ . For random variables $X$ and $Y$, define the join characteristic function of $X$, and $Y$ by $ phi_{X, Y}(s,t) = mathbb{E}[e^{i s X} e^{i t Y}]$. Then $X$ and $Y$ are independent iff | $$ phi_{X, Y}(s,t) = phi_X(s) phi_Y(t) text{ for all } s, t in mathbb{R} $$ . See chapter 5 of (Grimmett &amp; Stirzaker, 2001) for the statement and proof of the above facts. . Examples of characteristic functions . If $X sim operatorname{Ber}(p)$ then . $$ phi(t) = 1 - p + p e^{i t} $$ . If $X sim operatorname{Bin}(n,p)$ then . $$ phi(t) = (1 - p + p e^{i t} )^n $$ . If $X sim operatorname{NegBin}(r,p)$ then . $$ phi(t) = left ( frac{p}{1 - e^{it} + p e^{i t}} right )^r $$ . If $X sim operatorname{Gam}( alpha, beta)$ then . $$ phi(t) = left ( frac{ beta}{ beta - i t} right )^ alpha $$ . If $X sim operatorname{Exp}( lambda)$ then . $$ phi(t) = frac{ lambda}{ lambda - i t} $$ . If $X sim operatorname{Poi}( mu)$ then . $$ phi(t) = e^{ mu ( e^{i t} - 1)}$$ . If $X sim operatorname{N}( mu, sigma)$ then . $$ phi(t) = exp( i mu t - frac{ sigma t^2}{2}) $$ . If $X sim U(a, b)$ (continuous Uniform random variable) then . $$ phi(t) = frac{e^{i t b} - e^{i t a}}{ i t (b - a)} $$ . See section 5.8 in (Grimmett &amp; Stirzaker, 2001) for more details. Those functions, plus the above facts about characteristic functions, and the following . $$ operatorname{var}[X] = mathbb{E}[X^2] - ( mathbb{E}[X])^2 $$ . can be used to calculate $ mathbb{E}[X]$ and $ operatorname{var}[X]$. . The above formulas also clearly show which distributions are closed under taking sums of independent copies. . Normal distribution, weak law of large numbers, and the central limit theorem . We&#39;ll now study the normal distribution as the error distribution in the central limit theorem. . First recall that the normal distribution has the probability distribution: . $$ f(x) = frac{1}{ sigma sqrt{2 pi} } e^{- frac{1}{2} left ( frac{x- mu}{ sigma} right)^2}$$ . where $ mu in mathbb{R}$ and $ sigma &gt; 0$, and the standard moments are . $$ mathbb{E}[X] = mu, : : mathbb{E}[(X - mathbb{E}[X] )^2] = sigma$$ $$ operatorname{mode}{X} = mu $$ . Now consider a sequence $X_1$, $X_2$, ... of independent and identically distributed random variables each with mean $ mu$ and standard deviation $ sigma$ (good examples being the Bernoulli and Exponential distributions). Let $S_n = X_1 + X_2 + dots + X_n$ be their partial sums, then the weak law of large numbers1 says that $ frac{S_n}{n}$ converges to $ mu$ in distribution as $n$ approaches $ infty$ (see section 5.10 in (Grimmett &amp; Stirzaker, 2001)). We&#39;ll illustrate this theorem by way of example below. . lam = 0.5 mu = 2 def draw(n): return np.random.exponential(mu, size=n) . Below we&#39;ll plot the distribution of $ frac{S_n}{n}$ for $n = 1, 100$ and $1000$ to see the weak law in action. . fig = plt.figure(figsize=(10,10)) n = 1 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(221) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(&quot;Distribution of a random variable&quot;) n = 100 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(222) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Average of {n} IID random variables&quot;) n = 1000 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(223) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Average of {n} IID random variables&quot;) fig.suptitle(&#39;Illustration of the Weak Law of Large Numbers&#39;) plt.show() . It clearly looks like the errors in $ frac{S_n}{n} - mu$ are approaching a normal distribution, in fact $(S_n - n mu)/ sqrt{n sigma^2}$ approaches $N(0,1)$ in distribution as $n$ approaches $ infty$ (provided the $X_i$s has finite non-zero variance, (Grimmett &amp; Stirzaker, 2001)). This fact is the famous central limit theorem, which we&#39;ll illustrate below. . fig = plt.figure(figsize=(10,10)) n = 1 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(221) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(&quot;Distribution of a random variable&quot;) n = 100 y = np.array([ (sum(draw(n)) - mu * n)/np.sqrt(n) for i in range(1000) ] ) ax = fig.add_subplot(222) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Errors of {n} IID random variables&quot;) n = 1000 y = np.array([ (sum(draw(n)) - mu * n)/np.sqrt(n) for i in range(1000) ] ) ax = fig.add_subplot(223) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Errors of {n} IID random variables&quot;) fig.suptitle(&#39;Illustration of the Central Limit Theorem&#39;) plt.show() . Note that it&#39;s straightforward to check that . $$ mathbb{E}[(S_n - n mu)/ sqrt{n sigma^2}] = 0, : : operatorname{var}[(S_n - n mu)/ sqrt{n sigma^2}] = 1$$ . for all $ n &gt; 0 $. So the first two moments of $(S_n - n mu)/ sqrt{n sigma^2}$ agree with the first two moments of $N(0, 1)$. The central limit theorem then says that in the limit of large $n$, all moments of $(S_n - n mu)/ sqrt{n sigma^2}$ agree with all moments of $N(0, 1)$. . Observe that these theorems naturally apply to the Bernoulli and Exponential distributions in the limit of large $n$, showing that after setting the mean to zero and scaling by the standard deviation, they both converge to the standard normal distribution. . Finally, note that the weak law of large numbers, and the central limit theorem can be proven fairly elegantly using charactierstic functions (see (Grimmett &amp; Stirzaker, 2001), for example). Also see chapter 15 in Introduction to probability at an advanced level for more applications of the central limit theorem. . More summaries of standard statistics can be found in stats cookbook. . 1. The strong law of large numbers refers to almost everywhere convergence of the random variables.↩ . Murphy, K. P. (2012). Machine learning - a probabilistic perspective. | Grimmett, G. R., &amp; Stirzaker, D. R. (2001). Probability and random processes. | .",
            "url": "https://krishanr.github.io/fastpages/statistics/2022/01/13/standard-probability-distributions.html",
            "relUrl": "/statistics/2022/01/13/standard-probability-distributions.html",
            "date": " • Jan 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". My name is Krishan Rajaratnam and I’m data scientist and mathematician. I’ve always had a strong interest in mathematics, science, and technology, and I’m able to combine many of these interests in a career applying machine learning. . I finished my PhD at the University of Toronto under Professor Israel Michael Sigal studying the mathematical theory behind the Fractional Quantum Hall Effect (thesis available here). I also did a Master’s thesis at the University of Waterloo under Professor Dong Eui Chang and Professor Raymond G. McLenaghan studying and developing the mathematical theory behind the orthogonal separation of the Hamilton-Jacobi equation on spaces of constant curvature (thesis available here). . Besides my technical interests, I generally have a growth mindset and am interested in staying fit, with hobbies like biking, running, and weight lifting as well as photography. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://krishanr.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://krishanr.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}