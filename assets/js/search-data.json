{
  
    
        "post0": {
            "title": "Contours in OpenCV",
            "content": "!pip install opencv-python==4.5.5.64 !pip install watermark . The official tutorial on contours in OpenCV can be found here. We&#39;ll summarize some important methods discussed in this notebook, using the image below as an example. . import cv2 as cv import numpy as np import matplotlib.pyplot as plt . def display_image(img): fig, ax = plt.subplots(figsize=(16,16)) ax.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB)) plt.show() img = cv.imread(&quot;FindingContours.png&quot;) display_image(img) . Normally before finding contours in an image the image has to be binarized using a threshold algorithm, or canny edge detection. But in this case the image is already binarized, as the following calculation shows. . print(&quot;Shape: &quot;, img.shape) # The image pixels only take 2 values, hence it&#39;s binarized. print(&quot;Values: &quot;, np.unique(img)) . Shape: (578, 1132, 3) Values: [ 0 255] . We&#39;ll first convert the image to gray scale. . gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) gray.shape . (578, 1132) . Now find contours using the cv.CHAIN_APPROX_NONE method. This requires more memory but the following methods tend to be more robust (this includes convexity checking and orientation finding methods). . contours, _ = cv.findContours(gray, cv.RETR_LIST, cv.CHAIN_APPROX_NONE) len(contours) . 10 . Label the contours so we can refer to them later. The first method we&#39;ll use below is contourArea to filter out noise. We&#39;ll also use moments to calculate the contours centroid. . mimg = img.copy() # Go through all non-trivial contours and label them. for i, contour in enumerate(contours): area = cv.contourArea(contour) if area &lt; 1e2 or area &gt; 1e5: continue moments = cv.moments(contour) cx, cy = int(moments[&quot;m10&quot;]/moments[&quot;m00&quot;]), int(moments[&quot;m01&quot;]/moments[&quot;m00&quot;]) mimg = cv.putText(mimg, str(i), (cx,cy), cv.FONT_HERSHEY_COMPLEX, 1, (255,0,0), 2, cv.LINE_AA) # Replace -1 with the contour index to draw a specific contour. cv.drawContours(mimg, contours, -1, (0,255,0), 3) display_image(mimg) . Now we&#39;ll calculate several contour properties, including convexity, area, perimeter and angle. An important detail to keep in mind is that image coordinates have clockwise orientation (in contrast with Cartesian coordinates which are counter-clockwise). So for example, an angle of 10 degrees, lies in the quadrant with positive $x$ and $y$ values. See this tutorial for further details. . . for i, contour in enumerate(contours): area = cv.contourArea(contour) if area &lt; 1e2 or area &gt; 1e5: continue # isContourConvex doesn&#39;t work so well and depends on the chain approx # method. isconvex = cv.isContourConvex(contour) perimeter = cv.arcLength(contour, True) # We&#39;ll use the angle returned by fitEllipse _, _, angle = cv.fitEllipse(contour) print(f&quot;{i} IsConvex: {isconvex}, Area: {area:.2f}, Perimeter: {perimeter:.2f}, Angle: {angle:.2f}&quot;) . 0 IsConvex: False, Area: 5852.00, Perimeter: 306.00, Angle: 90.00 1 IsConvex: False, Area: 7104.50, Perimeter: 485.49, Angle: 19.44 2 IsConvex: False, Area: 5920.00, Perimeter: 310.84, Angle: 45.00 3 IsConvex: False, Area: 10290.00, Perimeter: 382.68, Angle: 90.00 4 IsConvex: False, Area: 7119.50, Perimeter: 343.81, Angle: 90.00 5 IsConvex: False, Area: 6518.00, Perimeter: 382.74, Angle: 40.76 6 IsConvex: False, Area: 5357.50, Perimeter: 358.21, Angle: 0.00 7 IsConvex: False, Area: 3375.00, Perimeter: 276.65, Angle: 90.00 8 IsConvex: False, Area: 7056.00, Perimeter: 329.76, Angle: 90.00 9 IsConvex: False, Area: 15776.50, Perimeter: 492.66, Angle: 27.55 . Below we&#39;ll use boundingRect to draw bounding boxes around the contours. . for contour in contours: x,y,w,h = cv.boundingRect(contour) cv.rectangle(mimg, (x,y), (x+w, y+h), (0,0,255),2) display_image(mimg) . Now we&#39;ll consider another method called minAreaRect, which calculates the rectangle enclosing the contour with minimal area. . for contour in contours: area = cv.contourArea(contour) if area &lt; 1e2 or area &gt; 1e5: continue # rect is the following tuple: ( center (x,y), (width, height), angle of rotation ) rect = cv.minAreaRect(contour) # Need to get verticies to draw the rectangle. box = cv.boxPoints(rect) box = np.int0(box) cv.drawContours(mimg, [box], 0, (255,0,0), 2) display_image(mimg) . At this point we&#39;ve seen two methods that give an orientation of a contour. The first was fitEllipse, and then minAreaRect. Finally we&#39;ll use PCACompute21 (see Introduction to PCA) for a third method for obtaining the contour orientation and compare it with the former methods. . for i, contour in enumerate(contours): area = cv.contourArea(contour) if area &lt; 1e2 or area &gt; 1e5: continue print(f&quot;Contour {i}&quot;) _,_, angle_minor = cv.fitEllipse(contour) print(f&quot;fitEllipse angle: {angle_minor:.2f}&quot;) _, _, angle_rect = cv.minAreaRect(contour) print(f&quot;minAreaRect angle: {angle_rect:.2f}&quot;) mean = np.empty((0)) mean, eigenvectors, eigenvalues = cv.PCACompute2(contour.squeeze().astype(np.float64), mean) # Compare with angles from PCA. print(&quot;PCA angles&quot;, [ round(num, 2) for num in cv.phase(eigenvectors[:,0], eigenvectors[:,1], angleInDegrees=True).reshape(-1)] ) print(&quot;&quot;) . Contour 0 fitEllipse angle: 90.00 minAreaRect angle: 90.00 PCA angles [0.0, 90.0] Contour 1 fitEllipse angle: 98.17 minAreaRect angle: 19.44 PCA angles [10.81, 100.81] Contour 2 fitEllipse angle: 0.04 minAreaRect angle: 45.00 PCA angles [89.99, 359.99] Contour 3 fitEllipse angle: 0.09 minAreaRect angle: 90.00 PCA angles [90.05, 0.05] Contour 4 fitEllipse angle: 0.15 minAreaRect angle: 90.00 PCA angles [90.38, 0.38] Contour 5 fitEllipse angle: 0.03 minAreaRect angle: 40.76 PCA angles [90.02, 0.02] Contour 6 fitEllipse angle: 116.06 minAreaRect angle: 0.00 PCA angles [27.48, 117.48] Contour 7 fitEllipse angle: 0.01 minAreaRect angle: 90.00 PCA angles [90.05, 0.05] Contour 8 fitEllipse angle: 90.00 minAreaRect angle: 90.00 PCA angles [0.0, 90.0] Contour 9 fitEllipse angle: 109.28 minAreaRect angle: 27.55 PCA angles [17.31, 107.31] . Note that the PCA angles are the angles made by the semi-major and semi-minor axis of the PCA ellipse about the x-axis. The above calculations suggest that fitEllipse returns the angle made by the semi-minor axis of the PCA ellipse (also see here). Due to this simple geometric interpretation, we use the angle returned by fitEllipse to represent the orientation of the contour. . %load_ext watermark %watermark --iversions -v . Python implementation: CPython Python version : 3.9.7 IPython version : 7.31.0 cv2 : 4.5.5 numpy : 1.19.5 matplotlib: 3.5.1 . 1. Note that the contour was converted to floats before passing it to PCACompute2. This is a consistent pattern throughout opencv-python that the appropriate data type has to be used.↩ .",
            "url": "https://krishanr.github.io/fastpages/opencv/python/2022/09/19/opencv-contours.html",
            "relUrl": "/opencv/python/2022/09/19/opencv-contours.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Multiclass Segmentation Metrics",
            "content": "In my last post I showed how to use torchmetrics to implement segmentation metrics for the Oxford-IIIT pet segmentation dataset. We saw that in addition to the average keyword introduced in the pet breed classification post, the mdmc_average keyword is necessary to compute metrics for image data. . In this post we&#39;ll dive deeper into these metrics, explaining the two choices for the mdmc_average parameter, including global and samplewise, as well as giving recommendations for dealing with imbalanced datasets. . The examples below will look primarily at precision and $F1$ score, but note that these metrics can be replaced by recall, dice score, etc. . !pip install pytorch-lightning !pip install -U git+https://github.com/qubvel/segmentation_models.pytorch !pip install seaborn . import torch import functools import segmentation_models_pytorch as smp from torchmetrics.functional.classification import precision, f1_score from torchmetrics.classification import StatScores from sklearn import metrics # Set the seed for reproduciblity. torch.manual_seed(7) import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap . To better understand the metrics, we&#39;ll work with a $4$ class problem with $n = 100$ samples. Classes $0$ and $3$ will have a probability of occurence of $ frac{1}{15}$, class $1$ will have a probability of $ frac{2}{3}$, and class $2$ will have a probability of $ frac{1}{5}$. We can generate data having this distribution using torch.multinomial below. . weights = torch.tensor([1, 10, 3, 1], dtype=torch.float) num_classes = len(weights) shape = (100,1,256,256) size = functools.reduce(lambda x, y : x* y, shape) output = torch.multinomial(weights, size, replacement=True).reshape(shape) output[70:,:,:,:] = torch.zeros(30, *shape[1:]) target = torch.multinomial(weights, size, replacement=True).reshape(shape) target[70:,:,:,:] = torch.zeros(30, *shape[1:]) . For example, a subset of the output looks like: . output[0,:,:10,:10] . tensor([[[1, 1, 2, 1, 2, 2, 1, 1, 2, 2], [1, 1, 1, 1, 1, 3, 1, 1, 1, 1], [1, 1, 3, 2, 1, 2, 1, 1, 1, 1], [0, 1, 1, 2, 3, 1, 1, 1, 1, 2], [1, 0, 1, 1, 1, 1, 1, 1, 1, 3], [1, 1, 1, 2, 0, 1, 1, 0, 1, 1], [1, 1, 1, 0, 1, 1, 2, 1, 2, 1], [2, 1, 1, 1, 2, 1, 2, 1, 3, 2], [3, 1, 1, 3, 1, 2, 1, 1, 1, 1], [2, 3, 0, 1, 1, 1, 1, 2, 2, 1]]]) . First we can collapse the image dimensions, $H$ and $W$, and then calculate metrics as for multiclass classification. This is precisely what happens when we choose mdmc_average global. . precision(output, target,num_classes=num_classes,average=&quot;macro&quot;,mdmc_average=&quot;global&quot;).item() . 0.4517214596271515 . For comparisons sake, in scikit-learn we have: . metrics.precision_score(target.reshape((-1)).numpy(),output.reshape((-1)).numpy(), average=&quot;macro&quot;) . 0.4517214441963613 . Then the different options for average can be chosen, including micro, macro, and weighted. . In contrast, the image dimensions can be treated separately, which is called the macro-imagewise reduction: . For each image and class the confusion table is computed over all pixels in an image. | Then the metric is computed for each image and class, as if it were a binary classifier. | The metrics are finally averaged over the images and classes. | This is the most natural way to calculate metrics like the Jaccard index (intersection over union) for example. Unfortunately the jaccard index can&#39;t be calculated this way using torchmetrics. However the $F1$/Dice Score can be calculated using torchmetrics, and it&#39;s equivalent to the Jaccard index1: . f1_score(output, target,num_classes=num_classes,average=&quot;macro&quot;,mdmc_average=&quot;samplewise&quot;).item() . 0.2497853934764862 . However if we calculate the $F1$ score using the segmentation models library, we get: . tp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode=&#39;multiclass&#39;, num_classes=num_classes) smp.metrics.f1_score(tp, fp, fn, tn, reduction=&quot;macro-imagewise&quot;).item() . 0.47478538751602173 . This is because our dataset has many images with no targets (recall that we zeroed out several images). Thus the $F1$ score for non-background classes reduces to $ frac{0}{0}$. smp replaces occurences of $ frac{0}{0}$ by $1$, while torchmetrics replaces $ frac{0}{0}$ by $0$. If we pass zero_division=0 to the segmentation models library, we get the same value as torchmetrics: . tp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode=&#39;multiclass&#39;, num_classes=num_classes) smp.metrics.f1_score(tp, fp, fn, tn, reduction=&quot;macro-imagewise&quot;, zero_division=0).item() . 0.2497853934764862 . This we why we recommend avoiding mdmc_average equal to samplewise, and calculating the metrics like for regular multiclass classifiers instead. . In conclusion when dealing with balanaced datasets, accuracy using the micro average plus mdmc_average global is sufficient, while the $F1$ score with the weighted average plus mdmc_average global is more accurate for imbalanaced datasets. . 1. This fact is discussed further here↩ . References . Torchmetrics Quickstart | Multiclass and multilabel classification in scikit-learn | Segmentation Models Pytorch Metrics | .",
            "url": "https://krishanr.github.io/fastpages/pytorch/torchmetrics/segmentationmodels/2022/09/06/multiclass_segmentation_metrics.html",
            "relUrl": "/pytorch/torchmetrics/segmentationmodels/2022/09/06/multiclass_segmentation_metrics.html",
            "date": " • Sep 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Pet Segmentation",
            "content": "In this example, we will build an image segmentation model to segment the 3 different classes in the Oxford-IIIT Pet Segmentation Dataset. . We&#39;ll use Segmentation Models PyTorch which was introduced in an earlier post on Surface Defect Segmentation, but in this post we focus on Torchmetrics, which is a new library that has many metrics for classification/segmentation in Pytorch. Torchmetrics . Allows for easy computation over batches. | Is rigorously tested. | A standardized interface to increase reproduciblity. | And much more... | . Torchmetrics was already introduced for Pet Breed Classification, but in this post we&#39;ll describe the mdmc_average parameter which is relevant for higher dimensional image data. . First we&#39;ll install Torchmetrics with PyTorch Lightning below. . !pip install pytorch_lightning !pip install -U git+https://github.com/qubvel/segmentation_models.pytorch . import os import random import collections import numpy as np import torch import torchvision import matplotlib.pyplot as plt # for plotting plt.rcParams[&quot;figure.figsize&quot;] = (10.0, 8.0) # set default size of plots plt.rcParams[&quot;font.size&quot;] = 16 from pytorch_lightning import LightningModule, Trainer, seed_everything import segmentation_models_pytorch as smp import torchvision.transforms.functional as TF from torch.utils.data import random_split seed_everything(7) . INFO:pytorch_lightning.utilities.seed:Global seed set to 7 . 7 . Exploratory Data Analysis . We&#39;ll first look at some image/mask pairs in the dataset and basic dataset statistics. To do this, we&#39;ll resize the images to a standard size of $224 times 224$. . def transforms(image,target): image, target = TF.resize(image,(256,256)), TF.resize(target,(256,256)) image, target = TF.center_crop(image,224), TF.center_crop(target, 224) # Shift the indicies so that they are from 0,...,num_classes-1 return TF.to_tensor(image), 255*TF.to_tensor(target) - 1 . vis_dataset = torchvision.datasets.OxfordIIITPet(root=&quot;./data&quot;, split=&quot;trainval&quot;, target_types=&quot;segmentation&quot;,transforms=transforms,download=True) . Downloading https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz Extracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet Downloading https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz Extracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet . As we shall see below, the segmentation masks have 3 labels (see the Cats and Dogs original paper): . Pet Body | Background | Ambiguous region (between body and background) | for i in range(5): sample_img, sample_msk = vis_dataset[random.choice(range(len(vis_dataset)))] plt.subplot(1,2,1) plt.title(&quot;Image&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_img.permute([1,2,0])) plt.subplot(1,2,2) plt.title(&quot;Mask&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_msk.permute([1,2,0]).squeeze()) plt.show() . Now we&#39;ll calculate the mask statistics. . vis_dataloader = torch.utils.data.DataLoader(vis_dataset, shuffle=False, batch_size=16, num_workers=os.cpu_count()) pixel_counts = collections.defaultdict(int) for _, mask in vis_dataloader: labels, counts = np.unique(mask.numpy(),return_counts=True) labels = list(map(int, labels)) for label, count in zip(labels,counts): pixel_counts[label] += count # Work with normalized counts pixel_counts = np.array(list(pixel_counts.values()))/sum(pixel_counts.values()) . As the figure below shows, our dataset is mildly imbalanced. Thus as mentioned in the Surface Defect Segmentation post, it makes sense to experiment with different loss functions offered by the segmentation models library. . fig, ax = plt.subplots(figsize=(8,5)) ax.barh(range(len(pixel_counts)), pixel_counts) width=0.15 ind = np.arange(3) ax.set_yticks(ind+width/2) ax.set_yticklabels([&quot;Body&quot;, &quot;Background&quot;, &quot;Border&quot;], minor=False) plt.xlabel(&quot;Number of pixels per class&quot;) plt.title(&quot;Distribution of pixel labels&quot;) plt.show() . Training . Define data augmentations to use with the train dataset. More data augmentations are possible with the albumentations library. . def train_transforms(image,target): # Only horizontal flips if random.random() &lt; 0.5: image = TF.hflip(image) target = TF.hflip(target) image, target = TF.resize(image,(256,256)), TF.resize(target,(256,256)) image, target = TF.center_crop(image,224), TF.center_crop(target, 224) # Shift the indicies so that they are from 0,...,num_classes-1 return TF.to_tensor(image), 255*TF.to_tensor(target) - 1 . def val_transforms(image,target): image, target = TF.resize(image,(256,256)), TF.resize(target,(256,256)) image, target = TF.center_crop(image,224), TF.center_crop(target, 224) # Shift the indicies so that they are from 0,...,num_classes-1 return TF.to_tensor(image), 255*TF.to_tensor(target) - 1 . train_dataset = torchvision.datasets.OxfordIIITPet(root=&quot;./data&quot;, split=&quot;trainval&quot;, target_types=&quot;segmentation&quot;,transforms=train_transforms,download=False) val_dataset = torchvision.datasets.OxfordIIITPet(root=&quot;./data&quot;, split=&quot;test&quot;, target_types=&quot;segmentation&quot;,transforms=train_transforms,download=False) . print(&#39;Length of train dataset: &#39;, len(train_dataset)) print(&#39;Length of validation dataset: &#39;, len(val_dataset)) . Length of train dataset: 3680 Length of validation dataset: 3669 . num_classes = 3 BATCH_SIZE = 16 . train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=os.cpu_count()) val_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=os.cpu_count()) . Now we&#39;ll subclass the LightningModule to create and train the model. The code below is similar to that in Pet Breed Classification and Surface Defect Segmentation, with the main difference being the metrics we define below. . We&#39;ll use the Accuracy and F1Score with torchmetrics to measure the performance of our model. The main difference from the pet breed classification example (which describes the average parameter) is that now we have to use the mdmc_average parameter to reduce the extra image dimensions, $H$ and $W$. We shall use mdmc_average=global, which is described in greater detail below. . For a given batch of data of shape $[B, C,H,W]$, the option mdmc_average=global collapses the data into shape $[B times H times W, C]$ and then calculates the F1Score as for multiclass classifiers. The option mdmc_average=samplewise on the other hand calculates the F1score for each of the $B$ samples and each of the $C$ classes, and then averages over the sample and class dimensions (cf. F1Score). The logic is similar for other metrics like the Dice score for example. This will be elaborated in an upcoming post, giving comparisons with the metrics in segmentation models pytorch, and recommendations for practical usage. . import torch.nn as nn from torchmetrics import MetricCollection, Accuracy, F1Score from torch.nn import functional as F class PetModel(LightningModule): def __init__(self, arch, encoder_name, learning_rate, num_classes, loss=&quot;DiceLoss&quot;, **kwargs): super().__init__() self.save_hyperparameters() self.example_input_array = torch.zeros((BATCH_SIZE, 3, 224,224)) # Setup the model. self.model = smp.create_model( arch, encoder_name=encoder_name, encoder_weights = &quot;imagenet&quot;, in_channels=3, classes=num_classes, **kwargs ) # Setup the losses. if loss == &quot;CrossEntropy&quot;: self.loss = nn.CrossEntropyLoss() else: self.loss = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True) # Setup the metrics. self.train_metrics = MetricCollection({&quot;train_acc&quot; : Accuracy(num_classes=num_classes, average=&quot;micro&quot;,mdmc_average=&quot;global&quot;), &quot;train_f1&quot; : F1Score(num_classes=num_classes,average=&quot;weighted&quot;,mdmc_average=&quot;global&quot;)}) self.val_metrics = MetricCollection({&quot;val_acc&quot; : Accuracy(num_classes=num_classes, average=&quot;micro&quot;,mdmc_average=&quot;global&quot;), &quot;val_f1&quot; : F1Score(num_classes=num_classes,average=&quot;weighted&quot;,mdmc_average=&quot;global&quot;)}) self.test_metrics = MetricCollection({&quot;test_acc&quot; : Accuracy(num_classes=num_classes, average=&quot;micro&quot;,mdmc_average=&quot;global&quot;), &quot;test_f1&quot; : F1Score(num_classes=num_classes,average=&quot;weighted&quot;,mdmc_average=&quot;global&quot;)}) def forward(self, x): return self.model(x) def training_step(self, batch, batch_idx): images, targets = batch #TODO: do this at dataset preparation. targets = targets.squeeze().long() logits_mask = self.forward(images) loss = self.loss(logits_mask, targets) preds = torch.softmax(logits_mask, dim=1) self.train_metrics(preds, targets) self.log(&quot;train_acc&quot;, self.train_metrics[&quot;train_acc&quot;], prog_bar=True) self.log(&quot;train_f1&quot;, self.train_metrics[&quot;train_f1&quot;], prog_bar=True) self.log(&quot;train_loss&quot;, loss, prog_bar=True) return loss def evaluate(self, batch, stage=None): images, targets = batch targets = targets.squeeze().long() logits_mask = self.forward(images) loss = self.loss(logits_mask, targets) preds = torch.softmax(logits_mask, dim=1) if stage == &quot;val&quot;: self.val_metrics(preds,targets) self.log(&quot;val_acc&quot;, self.val_metrics[&quot;val_acc&quot;], prog_bar=True) self.log(&quot;val_f1&quot;, self.val_metrics[&quot;val_f1&quot;], prog_bar=True) self.log(&quot;val_loss&quot;, loss, prog_bar=True) elif stage == &quot;test&quot;: self.test_metrics(preds,targets) self.log(&quot;test_acc&quot;, self.test_metrics[&quot;test_acc&quot;], prog_bar=True) self.log(&quot;test_f1&quot;, self.test_metrics[&quot;test_f1&quot;], prog_bar=True) self.log(&quot;test_loss&quot;, loss, prog_bar=True) def validation_step(self, batch, batch_idx): return self.evaluate(batch, &quot;val&quot;) def test_step(self, batch, batch_idx): return self.evaluate(batch, &quot;test&quot;) def configure_optimizers(self): return torch.optim.Adam(params=self.parameters(), lr=self.hparams.learning_rate) . We&#39;ll start off with the UNET architecture using a resnet34 backbone. Other options include using the DeepLabV3 architecture and the RegNetX backbone for slightly higher accuracy. . model = PetModel(&quot;UNET&quot;, &quot;resnet34&quot;, 1e-3, num_classes, loss=&quot;CrossEntropy&quot;) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . We&#39;ll log metrics to TensorBoard using the TensorBoardLogger, and save the best model, measured using F1Score, with the ModelCheckpoint. Note that we use the F1Score instead of Accuracy because of the mild class imbalance. . from pytorch_lightning.loggers import TensorBoardLogger from pytorch_lightning.callbacks import ModelCheckpoint name = &quot;oxfordpet&quot; + &quot;_&quot; + model.hparams.arch + &quot;_&quot; + model.hparams.encoder_name + &quot;_&quot; + model.hparams.loss logger = TensorBoardLogger(save_dir=&quot;lightning_logs&quot;, name=name, log_graph=True, default_hp_metric=False) callbacks = [ModelCheckpoint(monitor=&quot;val_f1&quot;,save_top_k=1, mode=&quot;max&quot;) ] . from itertools import islice def show_predictions_from_batch(model, dataloader, batch_num=0, limit = None): &quot;&quot;&quot; Method to visualize model predictions from batch batch_num. Show a maximum of limit images. &quot;&quot;&quot; batch = next(islice(iter(dataloader), batch_num, None), None) # Selects the nth item from dataloader, returning None if not possible. images, masks = batch with torch.no_grad(): model.eval() logits = model(images) pr_masks = torch.argmax(logits,dim=1) for i, (image, gt_mask, pr_mask) in enumerate(zip(images, masks, pr_masks)): if limit and i == limit: break fig = plt.figure(figsize=(15,4)) ax = fig.add_subplot(1,3,1) ax.imshow(image.squeeze().permute([1,2,0])) ax.set_title(&quot;Image&quot;) ax.axis(&quot;off&quot;) ax = fig.add_subplot(1,3,2) ax.imshow(gt_mask.squeeze()) ax.set_title(&quot;Ground truth&quot;) ax.axis(&quot;off&quot;) ax = fig.add_subplot(1,3,3) ax.imshow(pr_mask.squeeze()) ax.set_title(&quot;Predicted mask&quot;) ax.axis(&quot;off&quot;) . Sanity check the model by showing its predictions. . show_predictions_from_batch(model, val_dataloader, batch_num=3, limit=1) . Visualize training progress in TesnorBoard. . %load_ext tensorboard %tensorboard --logdir=./lightning_logs --bind_all . trainer = Trainer(accelerator=&#39;gpu&#39;, devices=1, max_epochs=12, logger=logger, callbacks=callbacks, fast_dev_run=False) . INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [&lt;class &#39;pytorch_lightning.callbacks.model_summary.ModelSummary&#39;&gt;]. Skipping setting a default `ModelSummary` callback. INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs . Finally, fit the model on the training dataset while saving the best model based on performance on the validation dataset. . trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader) . WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/oxfordpet_UNET_resnet34_CrossEntropy INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] INFO:pytorch_lightning.callbacks.model_summary: | Name | Type | Params | In sizes | Out sizes - 0 | model | Unet | 24.4 M | [16, 3, 224, 224] | [16, 3, 224, 224] 1 | loss | CrossEntropyLoss | 0 | ? | ? 2 | train_metrics | MetricCollection | 0 | ? | ? 3 | val_metrics | MetricCollection | 0 | ? | ? 4 | test_metrics | MetricCollection | 0 | ? | ? - 24.4 M Trainable params 0 Non-trainable params 24.4 M Total params 97.747 Total estimated model params size (MB) /usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can&#39;t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! if h % output_stride != 0 or w % output_stride != 0: INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=12` reached. . Visualize the model performance on the validation set. . show_predictions_from_batch(model, val_dataloader, batch_num=3, limit=5) . Analyze best saved model on the Validation dataset . best_model_path = trainer.checkpoint_callback.best_model_path print(best_model_path) . lightning_logs/oxfordpet_UNET_resnet34_CrossEntropy/version_0/checkpoints/epoch=10-step=2530.ckpt . best_model = PetModel.load_from_checkpoint(checkpoint_path=best_model_path) . trainer.test(best_model,dataloaders=val_dataloader) . INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Test metric DataLoader 0 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── test_acc 0.8975881338119507 test_f1 0.896770715713501 test_loss 0.27673694491386414 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── . [{&#39;test_acc&#39;: 0.8975881338119507, &#39;test_f1&#39;: 0.896770715713501, &#39;test_loss&#39;: 0.27673694491386414}] . show_predictions_from_batch(best_model, val_dataloader, batch_num=3, limit=5) .",
            "url": "https://krishanr.github.io/fastpages/pytorch/pytorchlightning/segmentationmodels/torchmetrics/torchvision/tensorboard/2022/08/25/pet_segmentation.html",
            "relUrl": "/pytorch/pytorchlightning/segmentationmodels/torchmetrics/torchvision/tensorboard/2022/08/25/pet_segmentation.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Multiclass Classification Metrics",
            "content": "In my last post I showed how to use torchmetrics to implement classification metrics for the Oxford-IIIT pet dataset. We saw that the average keyword had to be set to micro for accuracy and macro for the $F1$ score, so that the metrics were consistent with scikit-learn. In this post, I&#39;ll delve deeper into these keywords, and how they affect the metrics in question. In a following post, I&#39;ll also discuss mdmc_average, which is relevant for multiclass image segmentation. . Note that this keyword is relevant for binary classifiers, which are also mutliclass clasffiers with $2$ classes. . The examples below will look primarily at accuracy and precision, but note that precision can be replaced by recall, $F1$ score, jaccard index, etc. . !pip install pytorch-lightning !pip install seaborn . import torch import sklearn # Set the seed for reproduciblity. torch.manual_seed(7) import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap . To better understand the metrics, we&#39;ll work with a $4$ class problem with $n = 100$ samples. Classes $0$ and $3$ will have a probability of occurence of $ frac{1}{15}$, class $1$ will have a probability of $ frac{2}{3}$, and class $2$ will have a probability of $ frac{1}{5}$. We can generate data having this distribution using torch.multinomial below. . weights = torch.tensor([1, 10, 3, 1], dtype=torch.float) num_classes = len(weights) shape = (100,) size = shape[0] output = torch.multinomial(weights, size, replacement=True) target = torch.multinomial(weights, size, replacement=True) . For example, the output looks like: . output . tensor([1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 0, 0, 1, 0, 1, 2, 0, 3, 1, 3, 2, 2, 1, 1, 2, 1, 3, 2, 1, 3, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 3, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 3, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1]) . For our purposes, it&#39;ll be more convienent to work with one hot encoded data. . output_oh = torch.zeros(size, num_classes) output_oh[torch.arange(size), output] = 1 target_oh = torch.zeros(size, num_classes) target_oh[torch.arange(size), target] = 1 . Then the first $10$ samples of the output looks like: . output_oh[:10] . tensor([[0., 1., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 1., 0.]]) . Micro statistics . After the one hot encoding, the output and target tensors each have shape $[N, C]$. Due to this multidimensionality, there are at least two ways to compute the confusion matrix. For the first way, we calculate the confusion matrix of each sample over all classes: . from torchmetrics.classification import StatScores # Use reduce=&quot;samples&quot; to calculate the confusion matrix per sample. stat_scores = StatScores(num_classes=num_classes, reduce=&quot;samples&quot;) stats = stat_scores(output_oh, target) tp, fp, tn, fn, _ = stats[:, 0], stats[:, 1], stats[:, 2], stats[:, 3], stats[:, 4] . For example for the first $3$ samples, the confusion matricies are: . ax = sns.heatmap(torch.vstack([tp[:3], fp[:3], tn[:3], fn[:3]]).T.numpy(),annot=True, annot_kws={&quot;fontsize&quot;:12},linewidths=2, cbar=False,cmap=ListedColormap([&#39;white&#39;])) ax.set_xlabel(&quot;Stats&quot;, fontsize=16) ax.set_ylabel(&quot;Sample&quot;, fontsize=16) ax.set_title(&quot;Micro averaging&quot;, fontsize=16) ax.set_xticklabels([&quot;tp&quot;, &quot;fp&quot;, &quot;tn&quot;, &quot;fn&quot;], fontsize=12) plt.show() . The confusion matrix for the entire classifier is then obtained by summing over the samples: . ax = sns.heatmap(torch.tensor([[tp.sum(), fn.sum()], [fp.sum(), tn.sum()]]).numpy(),annot=True, annot_kws={&quot;fontsize&quot;:12},linewidths=2, cbar=False,cmap=ListedColormap([&#39;white&#39;])) ax.set_xlabel(&quot;Predicted&quot;, fontsize=16) ax.set_ylabel(&quot;Actual&quot;, fontsize=16) ax.set_yticklabels([&quot;T&quot;, &quot;F&quot;], fontsize=14) ax.set_xticklabels([&quot;P&quot;, &quot;N&quot;], fontsize=14) plt.show() . We could&#39;ve also reached the above answer using: . stat_scores = StatScores(num_classes=num_classes, reduce=&quot;micro&quot;) stats = stat_scores(output_oh, target) stats[:4] . tensor([ 40, 60, 240, 60]) . From which various metrics like accuracy, precision, and recall can be calculated. When these metrics are calculated this way, the averaging technique is called micro. For example, the accuracy, i.e. the number of correctly classified samples divided by the total samples, is . ((tp.sum())/(tp.sum()+fn.sum())).item() . 0.4000000059604645 . Similarly, using torchmetrics we get . from torchmetrics.functional.classification import accuracy accuracy(output_oh, target,num_classes=num_classes,average=&quot;micro&quot;).item() . 0.4000000059604645 . Which is precisely what scikit-learn calculates: . sklearn.metrics.accuracy_score(target, output) . 0.4 . Since the false positive and false negative counts are always the same, all other metrics like precision and recall are the same as accuracy. For example the precision is . ((tp.sum())/(tp.sum()+fp.sum())).item() . 0.4000000059604645 . Similarly, using torchmetrics we have: . from torchmetrics.functional.classification import precision precision(output_oh, target,num_classes=num_classes,average=&quot;micro&quot;).item() . 0.4000000059604645 . This is why micro statistics are rarely mentioned, because they don&#39;t give rise to new metrics. . Macro statistics . The second way to calculate the confusion matrix is to calculate the statistics for each class separately over all samples. . stat_scores = StatScores(num_classes=num_classes, reduce=&quot;macro&quot;) stats = stat_scores(output_oh, target) tp, fp, tn, fn, _ = stats[:,0], stats[:, 1], stats[:, 2], stats[:, 3], stats[:, 4] . which gives the confusion matricies for classes $0$, $1$,...,$3$. . ax = sns.heatmap(torch.vstack([tp, fp, tn, fn]).T.numpy(),annot=True, annot_kws={&quot;fontsize&quot;:12},linewidths=2, cbar=False,cmap=ListedColormap([&#39;white&#39;])) ax.set_xlabel(&quot;Stats&quot;, fontsize=16) ax.set_ylabel(&quot;Class&quot;, fontsize=16) ax.set_title(&quot;Macro averaging&quot;, fontsize=16) ax.set_xticklabels([&quot;tp&quot;, &quot;fp&quot;, &quot;tn&quot;, &quot;fn&quot;], fontsize=12) plt.show() . From which various metrics like precision, and recall can be calculated by calculating the metric for each class and then averaging. When these metrics are calculated this way, the averaging technique is called macro. For example, the precision, is . (tp/(tp + fp)).mean().item() . 0.20424403250217438 . Similarly, using torchmetrics we have: . from torchmetrics.functional.classification import precision precision(output,target,num_classes=num_classes,average=&quot;macro&quot;).item() . 0.20424403250217438 . which is the same as scikit-learn: . # of the second. sklearn.metrics.precision_score(target, output,average=&quot;macro&quot;) . 0.2042440318302387 . Finally, it&#39;s important to note that class $1$ is much more probable than the others, hence its precision score should be weighted differently than the others. In other words, when working with an imbalanced dataset like this one, it makes sense to use a weighted average: . precision(output,target,num_classes=num_classes,average=&quot;weighted&quot;).item() . 0.43180373311042786 . Which clearly paints a different picture of the quality of the predictions. . In conclusion when dealing with balanaced datasets, accuracy using the micro average is sufficient, while the $F1$ score with the weighted average is more accurate for imbalanaced datasets. . References . Torchmetrics Quickstart | Multiclass and multilabel classification in scikit-learn | .",
            "url": "https://krishanr.github.io/fastpages/pytorch/torchmetrics/classification/2022/08/18/multiclass_classification_metrics.html",
            "relUrl": "/pytorch/torchmetrics/classification/2022/08/18/multiclass_classification_metrics.html",
            "date": " • Aug 18, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Pet Breed Classification",
            "content": "In this example, we&#39;ll build an image classifier to detect the 37 different dog and cat breeds in the Oxford-IIIT Pet Dataset. This is a well studied example, so our main purpose here will be to illustrate its solution using Pytorch Lightning and Torchmetrics. . Torchmetrics is a new library that has many metrics for classification in Pytorch. . It allows for easy computation over batches. | Rigorously tested. | A standardized interface to increase reproduciblity. | And much more... | . We&#39;ll install it below with Pytorch Lightning. . !pip install pytorch-lightning !pip install lightning-bolts !pip install seaborn . import os import torch import random import numpy as np import pandas as pd import matplotlib.pyplot as plt from pytorch_lightning import LightningModule, Trainer, seed_everything from pl_bolts.transforms.dataset_normalizations import imagenet_normalization import torchvision import torchvision.transforms as transforms from torch.utils.data import random_split seed_everything(7) . INFO:pytorch_lightning.utilities.seed:Global seed set to 7 . 7 . Let&#39;s quickly load and visualize the dataset. A more thorough inspection can be done using Voxel 51. . vis_dataset = torchvision.datasets.OxfordIIITPet(root=&quot;./data&quot;,split=&quot;trainval&quot;,download=True) . Downloading https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz Extracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet Downloading https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz Extracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet . for i in range(5): image, label = vis_dataset[random.choice(range(len(vis_dataset)))] fig = plt.figure() plt.imshow(image) plt.title(f&quot;Image of a {vis_dataset.classes[label]}&quot;) plt.axis(&quot;off&quot;) . Now let&#39;s make transforms for the training, validation, and test set. We&#39;ll use torchvisions RandAugment which is an automated data augmentation strategy that improves the classifier accuracy by a few percentage points. . Also since we&#39;ll be using a ResNet model pretrained on ImageNet, we&#39;ll also use ImageNet normalization. . train_transform = transforms.Compose([ transforms.Resize((256,256)), transforms.CenterCrop(224), transforms.RandAugment(), transforms.ToTensor(), imagenet_normalization() ]) val_transform = transforms.Compose([ transforms.Resize((256,256)), transforms.CenterCrop(224), transforms.ToTensor(), imagenet_normalization() ]) . Most of the remaining steps are fairly standard for obtaining a fixed train/val/test dataset. Note that the split is fixed since we used seed_everything earlier. . train_dataset = torchvision.datasets.OxfordIIITPet(root=&quot;./data&quot;, split=&quot;trainval&quot;, transform=train_transform, download=False) test_dataset = torchvision.datasets.OxfordIIITPet(root=&quot;./data&quot;, split=&quot;test&quot;,transform=val_transform, download=False) . classes = train_dataset.classes num_classes = len(classes) BATCH_SIZE = 32 . split_len = int(0.9*len(train_dataset)) train_dataset, val_dataset = random_split(train_dataset, [split_len, len(train_dataset)-split_len], generator=torch.Generator().manual_seed(7)) . print(&#39;Length of train dataset: &#39;, len(train_dataset)) print(&#39;Length of validation dataset: &#39;, len(val_dataset)) print(&#39;Length of test dataset: &#39;, len(test_dataset)) . Length of train dataset: 3312 Length of validation dataset: 368 Length of test dataset: 3669 . train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=os.cpu_count()) val_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=os.cpu_count()) test_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=os.cpu_count()) . Let&#39;s take a look at the class statistics in the train dataset. . class_counts = np.array([0]*num_classes) for batch in train_dataloader: _, labels = batch for label in labels: class_counts[label] += 1 df_classes = pd.DataFrame({&quot;classes&quot; : classes, &quot;class_counts&quot; : class_counts}) . As we&#39;ll see the classes are balanced, which simplifies the treatment of this problem. . df_classes.plot(kind=&quot;barh&quot;, x=&quot;classes&quot;, y=&quot;class_counts&quot;,figsize=(8,12)) plt.show() . Now we&#39;ll write the Pytorch Lightning module to create and train the model. . To train our model, we&#39;ll freeze pretrained models from torchvision and add a linear head for fine tuning. For illustrative purposes, we&#39;ll use a ResNet34 model, but you can also try ResNext50, or RegNetX to obtain slightly higher accuracy. The model will be set as the model attribute in the LightningModule class. . Since the LightningModule inherits from the Pytorch Module, we also have to set the forward method of the neural network. The backward method is automatically generated. . The training is relatively simple, using an Adam optimizer, which is set in the configure_optimizers method. . The training, validation, and test steps for a batch of data are configured via the training_step, validation_step, and test_step respectively. We&#39;ll use a single custom method called evaluate to handle the validation and test steps. See the Lightning documentation for more details. . Finally, we&#39;ll use Accuracy and F1Score with torchmetrics to measure the performance of our model. . Since we&#39;re using the object oriented API, which preserves state, we have to create separate objects for the train, val and test sets. | We wrap Accuracy and F1Score in the MetricCollection, which will automatically share common computations between the two metrics, and simplify the code. | For multiclass classifiers, the average option passed to the metrics (micro or macro) affects the calculation. The choices below were chosen to be consistent with scikit-learn, and work well with a balanaced dataset. I&#39;ll delve deeper into these nuances in following posts. | . import torch.nn as nn from torchmetrics import MetricCollection, Accuracy, F1Score from torchvision.models import resnet34, ResNet34_Weights from torchvision.models import resnext50_32x4d, ResNeXt50_32X4D_Weights from torchvision.models import regnet_x_3_2gf, RegNet_X_3_2GF_Weights class PetModel(LightningModule): def __init__(self, arch, learning_rate, num_classes): super().__init__() self.save_hyperparameters() self.example_input_array = torch.zeros((BATCH_SIZE, 3, 224,224)) # Setup the model. self.create_model(arch) # Setup the losses. self.loss = nn.CrossEntropyLoss() # Setup the metrics. self.train_metrics = MetricCollection({&quot;train_acc&quot; : Accuracy(num_classes=num_classes, average=&quot;micro&quot;), &quot;train_f1&quot; : F1Score(num_classes=num_classes, average=&quot;macro&quot;)}) self.val_metrics = MetricCollection({&quot;val_acc&quot; : Accuracy(num_classes=num_classes, average=&quot;micro&quot;), &quot;val_f1&quot; : F1Score(num_classes=num_classes, average=&quot;macro&quot;)}) self.test_metrics = MetricCollection({&quot;test_acc&quot; : Accuracy(num_classes=num_classes, average=&quot;micro&quot;), &quot;test_f1&quot; : F1Score(num_classes=num_classes, average=&quot;macro&quot;)}) def create_model(self, arch): &quot;&quot;&quot; Setup a model for fine tuning. &quot;&quot;&quot; in_dimension = 512 if arch == &quot;resnext50_32x4d&quot;: self.model = resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.DEFAULT) in_dimension = 4*512 elif arch == &quot;regnet_x_3_2gf&quot;: self.model = regnet_x_3_2gf(weights=RegNet_X_3_2GF_Weights.DEFAULT) in_dimension = 1008 else: self.model = resnet34(weights=ResNet34_Weights.DEFAULT) in_dimension = 512 for param in self.model.parameters(): param.requires_grad = False self.model.fc = nn.Linear(in_dimension, num_classes) def forward(self, x): return self.model(x) def training_step(self, batch, batch_idx): images, labels = batch logits = self(images) preds = torch.argmax(logits, dim=1) loss = self.loss(logits, labels) self.train_metrics(preds, labels) self.log(&quot;train_acc&quot;, self.train_metrics[&quot;train_acc&quot;], prog_bar=True) self.log(&quot;train_f1&quot;, self.train_metrics[&quot;train_f1&quot;], prog_bar=True) self.log(&quot;train_loss&quot;, loss, prog_bar=True) return loss def evaluate(self, batch, stage=None): images, labels = batch logits = self(images) preds = torch.argmax(logits, dim=1) loss = nn.CrossEntropyLoss()(logits, labels) if stage == &quot;val&quot;: self.val_metrics(preds,labels) self.log(&quot;val_acc&quot;, self.val_metrics[&quot;val_acc&quot;], prog_bar=True) self.log(&quot;val_f1&quot;, self.val_metrics[&quot;val_f1&quot;], prog_bar=True) self.log(&quot;val_loss&quot;, loss, prog_bar=True) elif stage == &quot;test&quot;: self.test_metrics(preds,labels) self.log(&quot;test_acc&quot;, self.test_metrics[&quot;test_acc&quot;], prog_bar=True) self.log(&quot;test_f1&quot;, self.test_metrics[&quot;test_f1&quot;], prog_bar=True) self.log(&quot;test_loss&quot;, loss, prog_bar=True) def validation_step(self, batch, batch_idx): return self.evaluate(batch, &quot;val&quot;) def test_step(self, batch, batch_idx): return self.evaluate(batch, &quot;test&quot;) def configure_optimizers(self): return torch.optim.Adam(params=self.parameters(), lr=self.hparams.learning_rate) . model = PetModel(&quot;resnet34&quot;, 1e-3, num_classes) . We&#39;ll log metrics to TensorBoard using the TensorBoardLogger, and save the best model, measured using accuracy, with the ModelCheckpoint. . from pytorch_lightning.loggers import TensorBoardLogger from pytorch_lightning.callbacks import ModelCheckpoint name = &quot;oxfordpet&quot; + &quot;_&quot; + model.hparams.arch logger = TensorBoardLogger(save_dir=&quot;lightning_logs&quot;, name=name, log_graph=True, default_hp_metric=False) callbacks = [ModelCheckpoint(monitor=&quot;val_acc&quot;,save_top_k=1, mode=&quot;max&quot;) ] . Initialize the trainer by requiring training to be done on the GPU, max epochs of 12, setting the logger, and callbacks. . trainer = Trainer(accelerator=&#39;gpu&#39;, devices=1, max_epochs=12, logger=logger, callbacks=callbacks, fast_dev_run=False) . INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [&lt;class &#39;pytorch_lightning.callbacks.model_summary.ModelSummary&#39;&gt;]. Skipping setting a default `ModelSummary` callback. INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs . Visualize training progress in TesnorBoard. . %load_ext tensorboard %tensorboard --logdir=./lightning_logs --bind_all . Finally, fit the model on the training dataset while saving the best model based on performance on the validation dataset. . trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader) . WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/oxfordpet_resnet34 INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] INFO:pytorch_lightning.callbacks.model_summary: | Name | Type | Params | In sizes | Out sizes -- 0 | model | ResNet | 21.3 M | [32, 3, 224, 224] | [32, 37] 1 | loss | CrossEntropyLoss | 0 | ? | ? 2 | train_metrics | MetricCollection | 0 | ? | ? 3 | val_metrics | MetricCollection | 0 | ? | ? 4 | test_metrics | MetricCollection | 0 | ? | ? -- 19.0 K Trainable params 21.3 M Non-trainable params 21.3 M Total params 85.215 Total estimated model params size (MB) INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=12` reached. . Let&#39;s load the best model and measure its performance. . best_model_path = trainer.checkpoint_callback.best_model_path print(best_model_path) . lightning_logs/oxfordpet_resnet34/version_0/checkpoints/epoch=7-step=832.ckpt . best_model = PetModel.load_from_checkpoint(checkpoint_path=best_model_path) . trainer.test(best_model,dataloaders=test_dataloader) . INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Test metric DataLoader 0 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── test_acc 0.8396739363670349 test_f1 0.8321808576583862 test_loss 0.4552876353263855 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── . [{&#39;test_acc&#39;: 0.8396739363670349, &#39;test_f1&#39;: 0.8321808576583862, &#39;test_loss&#39;: 0.4552876353263855}] . Now visualize the confusion matrix. . from torchmetrics import ConfusionMatrix confmat = ConfusionMatrix(num_classes=num_classes, normalize=&#39;true&#39;) prev_device = best_model.device.type device = torch.device(&quot;cuda&quot; if torch.cuda.is_available else &quot;cpu&quot;) confmat.to(device) best_model.to(device) with torch.no_grad(): best_model.eval() for batch in test_dataloader: images, labels = batch images, labels = images.to(device), labels.to(device) logits = best_model(images) preds = torch.argmax(logits, dim=1) confmat.update(preds, labels) best_model.to(prev_device) 1 . 1 . import seaborn as sns cmat = confmat.compute().cpu().numpy() fig, ax = plt.subplots(figsize=(20,20)) df_cm = pd.DataFrame(cmat, index = range(len(classes)), columns=range(len(classes))) ax = sns.heatmap(df_cm, annot=True, fmt=&#39;.2g&#39;, cmap=&#39;Spectral&#39;) ax.set_yticklabels([classes[i] for i in range(len(classes))], rotation=0) ax.set_xticklabels([classes[i] for i in range(len(classes))], rotation=90) ax.set_xlabel(&quot;Predicted label&quot;) ax.set_ylabel(&quot;Actual label&quot;) plt.show(block=False) .",
            "url": "https://krishanr.github.io/fastpages/pytorch/pytorchlightning/torchmetrics/torchvision/tensorboard/2022/08/17/pet_breed_classification.html",
            "relUrl": "/pytorch/pytorchlightning/torchmetrics/torchvision/tensorboard/2022/08/17/pet_breed_classification.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Surface Defect Segmentation",
            "content": "In this notebook we use the segmentation models library built in PyTorch to train image segmentation models for the Magnetic tile defect dataset. This dataset is interesting because it is highly imbalanced, with less than 1% of pixels corresponding to the target class. Using the segmentation models library we can try several different loss functions, including binary cross entropy, focal losss, and Tversky loss to see their performance (see here for recommended loss functions). However, it turns out that using binary cross entropy is sufficient to get good results. . Aditionally, following an example in the segmentation models library, we&#39;ll use PyTorch Lightning to further simplify the training process in PyTorch and MLflow to log hyperparameters and metrics. . This code is built with the help of Detection of Surface Defects in Magnetic Tile Images by Dr. Mitra P. Danesh. . To install the required packages, usually it&#39;s sufficient to run the cell below (for example on Google Colab or Paperspace using the PyTorch runtime). If PyTorch is not installed, then uncomment the first line of the cell below to install it. . #!pip install torch torchvision !pip install -U git+https://github.com/qubvel/segmentation_models.pytorch !pip install pytorch-lightning !pip install mlflow . import os import math import random import torch import numpy as np import segmentation_models_pytorch as smp import pytorch_lightning as pl from torch.utils.data import DataLoader from torch.utils.data import Dataset as BaseDataset from pytorch_lightning import Trainer from pytorch_lightning.loggers import MLFlowLogger # set the random seeds for reproducibility random.seed(42) torch.manual_seed(0) np.random.seed(0) . Loading data . from torch.utils.data import random_split import torchvision.transforms as transforms from torch.utils.data import Dataset from PIL import Image from glob import glob . First download the data locally. . %%capture !wget -O data.zip https://github.com/abin24/Magnetic-tile-defect-datasets./archive/master.zip !unzip data.zip !mv Magnetic-tile-defect-datasets.-master data . classes =[&#39;Blowhole&#39;, &#39;Crack&#39;,&#39;Free&#39;] # classes/labels image_paths = [] for c in classes: # retreive image file paths recursively images_found = glob(&#39;data/MT_&#39; + c + &#39;/Imgs/*.jpg&#39;,recursive=True) if c== &#39;Free&#39;: # undersample the free class. image_paths.extend( images_found[:80] ) else: image_paths.extend( images_found ) random.shuffle(image_paths) . len(image_paths) . 252 . Dataset . Writing helper class for data extraction, tranformation and preprocessing https://pytorch.org/docs/stable/data. Also see the binary segmentation intro in the segmentation models library for more details on designing the Dataset class. For more sophisticated data augmentations, see the albumentations library and specifically this.ipynb) notebook from segmentation models. . import torchvision.transforms.functional as TF import random class SurfaceDefectDetectionDataset(Dataset): def __init__(self, image_path_list, use_transform=False): super().__init__() self.image_path_list = image_path_list self.use_transform = use_transform def transform(self, image, target): if random.random() &lt; 0.5: image = TF.hflip(image) target = TF.hflip(target) if random.random() &lt; 0.5: image = TF.vflip(image) target = TF.vflip(target) angle = random.choice([0, -90, 90, 180]) image, target = TF.rotate(image, angle), TF.rotate(target, angle) return image, target def __len__(self): return len(self.image_path_list) def __getitem__(self, idx): # Open the image file which is in jpg image = Image.open(self.image_path_list[idx]) # The mask is in png. # Use the image path, and change its extension to png to get the mask&#39;s path. mask = Image.open(os.path.splitext(self.image_path_list[idx])[0]+&#39;.png&#39;) # resize the images. image, mask = TF.resize(image, (320,320)), TF.resize(mask, (320,320)) # Perform augmentation if required. if self.use_transform: image, mask = self.transform(image, mask) # Transform the image and mask PILs to torch tensors. image, mask = TF.to_tensor(image), TF.to_tensor(mask) # Threshold mask, threshold limit is 0.5 mask = (mask &gt;= 0.5)*(1.0) #return the image and mask pair tensors return image, mask . split_len = int(0.8*len(image_paths)) train_dataset = SurfaceDefectDetectionDataset(image_paths[:split_len], use_transform = True) test_dataset = SurfaceDefectDetectionDataset(image_paths[split_len:], use_transform = False) . We&#39;ll randomly split the train and validation set, but fix the random seed to fix these datasets. . train_dataset, val_dataset = random_split(train_dataset, [int(split_len*0.9), split_len - int(split_len*0.9)], generator=torch.Generator().manual_seed(1)) . print(&#39;Length of train dataset: &#39;, len(train_dataset)) print(&#39;Length of validation dataset: &#39;, len(val_dataset)) print(&#39;Length of test dataset: &#39;, len(test_dataset)) . Length of train dataset: 180 Length of validation dataset: 21 Length of test dataset: 51 . Let&#39;s take a look at the dataset . import matplotlib.pyplot as plt import random sample_img, sample_msk = train_dataset[random.choice(range(len(train_dataset)))] plt.subplot(1,2,1) plt.title(&quot;Sample from trainining set&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_img.squeeze(), cmap=&#39;gray&#39;) plt.subplot(1,2,2) plt.axis(&quot;off&quot;) plt.imshow(sample_msk.squeeze(), cmap=&#39;gray&#39;) plt.show() sample_img, sample_msk = val_dataset[random.choice(range(len(val_dataset)))] plt.subplot(1,2,1) plt.title(&quot;Sample from validation set&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_img.squeeze(), cmap=&#39;gray&#39;) plt.subplot(1,2,2) plt.axis(&quot;off&quot;) plt.imshow(sample_msk.squeeze(), cmap=&#39;gray&#39;) plt.show() sample_img, sample_msk = test_dataset[random.choice(range(len(test_dataset)))] plt.subplot(1,2,1) plt.title(&quot;Sample from test set&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_img.squeeze(), cmap=&#39;gray&#39;) plt.subplot(1,2,2) plt.axis(&quot;off&quot;) plt.imshow(sample_msk.squeeze(), cmap=&#39;gray&#39;) plt.show() . Take a look at more samples from the train set. . for i in range(3): sample_img, sample_msk = train_dataset[random.choice(range(len(train_dataset)))] plt.subplot(1,2,1) plt.title(&quot;Image&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_img.squeeze(), cmap=&#39;gray&#39;) plt.subplot(1,2,2) plt.title(&quot;Mask&quot;) plt.axis(&quot;off&quot;) plt.imshow(sample_msk.squeeze(), cmap=&#39;gray&#39;) plt.show() . Find the weight of positive and negative pixels. . The number of positive pixels is less than 1% of the total, showing that the dataset is highly imbalanced. . positive_weight = 0 negative_weight = 0 total_pixels = 0 img_shape = train_dataset[0][0].shape for _, target in train_dataset: positive_weight += (target &gt;= 0.5).sum().item() negative_weight += (target &lt; 0.5).sum().item() total_pixels += (img_shape[1] * img_shape[2]) positive_weight /= total_pixels negative_weight /= total_pixels print(&#39;positive weight = &#39;,positive_weight, &#39; tnegative weight = &#39;, negative_weight) . positive weight = 0.002119411892361111 negative weight = 0.9978805881076389 . Create model and train . from itertools import islice def show_predictions_from_batch(model, dataloader, batch_num=0, limit = None): &quot;&quot;&quot; Method to visualize model predictions from batch batch_num. Show a maximum of limit images. &quot;&quot;&quot; batch = next(islice(iter(dataloader), batch_num, None), None) # Selects the nth item from dataloader, returning None if not possible. images, masks = batch with torch.no_grad(): model.eval() logits = model(images) pr_masks = logits.sigmoid() pr_masks = (pr_masks &gt;= 0.5)*1 for i, (image, gt_mask, pr_mask) in enumerate(zip(images, masks, pr_masks)): if limit and i == limit: break fig = plt.figure(figsize=(15,4)) ax = fig.add_subplot(1,3,1) ax.imshow(image.squeeze(), cmap=&#39;gray&#39;) ax.set_title(&quot;Image&quot;) ax.axis(&quot;off&quot;) ax = fig.add_subplot(1,3,2) ax.imshow(gt_mask.squeeze(), cmap=&#39;gray&#39;) ax.set_title(&quot;Ground truth&quot;) ax.axis(&quot;off&quot;) ax = fig.add_subplot(1,3,3) ax.imshow(pr_mask.squeeze(), cmap=&#39;gray&#39;) ax.set_title(&quot;Predicted mask&quot;) ax.axis(&quot;off&quot;) . We&#39;ll create a PyTorch Lightning module to help streamline the training process. In the class initalization, it uses the segmentation models library, via the call to smp.create_model, to build a PyTorch model which operates on one channel images for binary segmentation. Many state of the art models are possible with the segmentation models library. However, we&#39;ll typically use the Unet with resnet34 backbone. . We use the SoftBCEWithLogitsLoss to train the model, but other losses recommended here, can easily be used as well. . We&#39;ll also use the segmentation models library to monitor the intersection over union metric. Due to the highly imbalanced nature of this dataset, this will give us a much better indicator of model quality than the accuracy. . class SurfaceDefectModel(pl.LightningModule): def __init__(self, arch, encoder_name, loss = &quot;SoftBCEWithLogitsLoss&quot; , **kwargs): super().__init__() self.model = smp.create_model( arch, encoder_name=encoder_name, encoder_weights = None, in_channels=1, classes=1, **kwargs ) self.arch = arch self.encoder_name = encoder_name self.loss_name = loss if loss == &quot;DiceLoss&quot;: self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True) elif loss == &quot;TverskyLoss&quot;: self.loss_fn = smp.losses.TverskyLoss(smp.losses.BINARY_MODE, from_logits=True, alpha=0.3,beta=0.7) elif loss == &quot;FocalLoss&quot;: self.loss_fn = smp.losses.FocalLoss(smp.losses.BINARY_MODE) else: self.loss_fn = smp.losses.SoftBCEWithLogitsLoss() self.printed_run_id = None self.run_id = None def forward(self, image): return self.model(image) def shared_step(self, batch, stage): image = batch[0] # Shape of the image should be (batch_size, num_channels, height, width) # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width] assert image.ndim == 4 # Check that image dimensions are divisible by 32, # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -&gt; 5, 10, 20, 40, 80 # and we will get an error trying to concat these features h, w = image.shape[2:] assert h % 32 == 0 and w % 32 == 0 mask = batch[1] # Shape of the mask should be [batch_size, num_classes, height, width] # for binary segmentation num_classes = 1 assert mask.ndim == 4 # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation assert mask.max() &lt;= 1.0 and mask.min() &gt;= 0 logits_mask = self.forward(image) # Predicted mask contains logits, and loss_fn param `from_logits` is set to True loss = self.loss_fn(logits_mask, mask) # Lets compute metrics for some threshold # first convert mask values to probabilities, then # apply thresholding prob_mask = logits_mask.sigmoid() pred_mask = (prob_mask &gt; 0.5).float() # We will compute IoU metric by two ways # 1. dataset-wise # 2. image-wise # but for now we just compute true positive, false positive, false negative and # true negative &#39;pixels&#39; for each image and class # these values will be aggregated in the end of an epoch tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=&quot;binary&quot;) return { &quot;loss&quot;: loss, &quot;tp&quot;: tp, &quot;fp&quot;: fp, &quot;fn&quot;: fn, &quot;tn&quot;: tn, } def shared_epoch_end(self, outputs, stage): # aggregate step metics tp = torch.cat([x[&quot;tp&quot;] for x in outputs]) fp = torch.cat([x[&quot;fp&quot;] for x in outputs]) fn = torch.cat([x[&quot;fn&quot;] for x in outputs]) tn = torch.cat([x[&quot;tn&quot;] for x in outputs]) # per image IoU means that we first calculate IoU score for each image # and then compute mean over these scores per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=&quot;micro-imagewise&quot;) # dataset IoU means that we aggregate intersection and union over whole dataset # and then compute IoU score. The difference between dataset_iou and per_image_iou scores # in this particular case will not be much, however for dataset # with &quot;empty&quot; images (images without target class) a large gap could be observed. # Empty images influence a lot on per_image_iou and much less on dataset_iou. dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=&quot;micro&quot;) accuracy = smp.metrics.accuracy(tp, fp, fn, tn) metrics = { f&quot;{stage}_per_image_iou&quot;: per_image_iou, f&quot;{stage}_dataset_iou&quot;: dataset_iou, f&quot;{stage}_accuracy&quot;: accuracy, f&quot;{stage}_loss&quot;: torch.tensor([x[&quot;loss&quot;].item() for x in outputs]).mean() } # Log the metrics #for key, val in metrics.items(): # self.logger.experiment.log_metric(self.logger.run_id ,key, val.mean().item(), step=self.current_epoch) self.logger.log_metrics({key: val.mean().item() for key, val in metrics.items() }, step=self.current_epoch) # only record the loss in mlflow del metrics[f&quot;{stage}_loss&quot;] if not self.printed_run_id and hasattr(self.logger, &quot;run_id&quot;): print(&#39;Run id: &#39;, self.logger.run_id ) self.printed_run_id = True # This will be available in tensorboard. self.log_dict(metrics, prog_bar=True) def training_step(self, batch, batch_idx): return self.shared_step(batch, &quot;train&quot;) def training_epoch_end(self, outputs): self.shared_epoch_end(outputs, &quot;train&quot;) def validation_step(self, batch, batch_idx): return self.shared_step(batch, &quot;valid&quot;) def validation_epoch_end(self, outputs): self.shared_epoch_end(outputs, &quot;valid&quot;) def test_step(self, batch, batch_idx): return self.shared_step(batch, &quot;test&quot;) def test_epoch_end(self, outputs): return self.shared_epoch_end(outputs, &quot;test&quot;) def configure_optimizers(self): return torch.optim.Adam(self.parameters(), lr=0.0001) def on_fit_end(self): # Log hyperparameters to mlflow. self.logger.log_hyperparams({ &quot;arch&quot;: self.arch, &quot;encoder_name&quot;: self.encoder_name, &quot;loss&quot;: self.loss_name }) if hasattr(self.logger, &quot;run_id&quot;): self.run_id = self.logger.run_id . Note that we added the extra hook on_fit_end to save hyperparameters to MLflow. More hooks are available at the official documentation lightning hooks. . batch_size = 8 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2) valid_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2) . model = SurfaceDefectModel(&quot;Unet&quot;, &quot;resnet34&quot;) . Sanity check the model by showing its predictions. . show_predictions_from_batch(model, train_loader, batch_num=0, limit=1) . We&#39;ll use the ModelCheckpoint callback from PyTorch lightning to save the best model, as measured by the intersection over union metric. . from pytorch_lightning.callbacks import ModelCheckpoint from pathlib import Path checkpoint_callback = ModelCheckpoint( monitor=&quot;valid_dataset_iou&quot;, dirpath=&quot;./models&quot;, filename= f&quot;surface_defect_{model.arch}_{model.encoder_name}_{model.loss_name}&quot;, save_top_k=3, mode=&quot;max&quot;, ) # Add the model directory if it it doesn&#39;t exist Path(&quot;./models&quot;).mkdir(exist_ok=True) . Now with the help of PyTorch lightning, we can train and log to MLflow, with a few lines of code. . mlf_logger = MLFlowLogger(experiment_name=&quot;lightning_logs&quot;) trainer = pl.Trainer( gpus=1, max_epochs=200, log_every_n_steps=math.ceil(len(train_dataset)/batch_size), callbacks=[checkpoint_callback], logger=mlf_logger, # For debugging purposes, uncomment the line below. #fast_dev_run=True ) trainer.fit( model, train_dataloaders=train_loader, val_dataloaders=valid_loader, ) . Load the best model to analyze its performance. . model.load_from_checkpoint(f&quot;models/surface_defect_{model.arch}_{model.encoder_name}_{model.loss_name}.ckpt&quot;, arch = model.arch, encoder_name= model.encoder_name, loss = model.loss_fn.__class__.__name__) . trainer.validate(model, dataloaders=valid_loader, verbose=False) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . [{&#39;valid_accuracy&#39;: 0.9991360902786255, &#39;valid_dataset_iou&#39;: 0.6411741971969604, &#39;valid_per_image_iou&#39;: 0.7908229827880859}] . Visualize the model performance on the validation set. . for i in range(min(len(valid_loader), 5)): show_predictions_from_batch(model, valid_loader, batch_num=i) . Analyze best saved model on the Test set . test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2) . trainer.test(model, dataloaders=test_loader, verbose=False) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . [{&#39;test_accuracy&#39;: 0.9993589520454407, &#39;test_dataset_iou&#39;: 0.6736205816268921, &#39;test_per_image_iou&#39;: 0.7249590158462524}] . Finally, visualize the model performance on the test set. . show_predictions_from_batch(model, test_loader, batch_num=0) .",
            "url": "https://krishanr.github.io/fastpages/pytorch/segmentationmodels/pytorchlightning/mlflow/2022/06/30/surface-defect-segmentation.html",
            "relUrl": "/pytorch/segmentationmodels/pytorchlightning/mlflow/2022/06/30/surface-defect-segmentation.html",
            "date": " • Jun 30, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "How to build a Docker image with an Anaconda environment in Paperspace?",
            "content": "Paperspace is an affordable tool for obtaining cloud compute power. Using it, you can run jupyter notebooks, python scripts, or just about anything, provided you can write the right Docker file. Plus some of the compute instances also come with GPUs, making Paperspace an alternative to Google Colab. In contrast with Google Colab though, Paperspace saves the files you use, so they are there when you return. Inspired by Alex&#39;s post on Building my own image to use IceVision with Paperspace, in this post I&#39;ll describe how you can build a docker image to wrap an Anaconda environment, and then run it on Paperspace. The Dockerfile and Anaconda environment I used are available on Github here. . The context for this post is that even though a custom anaconda environment can be created and run in the Paperspace images that are readily available, for longer projects, it would be useful to use a Paperspace image that comes prebuilt with a desired environment. . We can do this by adding a few additional lines to the standard Paperspace Docker image. Any anconda environment available at &#39;environment.yml&#39; can be created and activated using these 4 lines: . ADD environment.yml $APP_HOME/environment.yml RUN conda env create -f $APP_HOME/environment.yml ENV PATH $CONDA_DIR/envs/tf2/bin:$PATH RUN echo &quot;source activate tf2&quot; &gt; ~/.bashrc . The first two commands add the conda environment file to the Docker containers workspace, and then create the conda environment. . The third command adds the enviornment directory to the PATH enviroment variable. The directory path contains the environment name, which in this case is tf2 (replace this with your environment name if necessary). Finally, the last command adds a command to activate the enviornment to the bashrc file, so that the environment is activated when the container is started. . That&#39;s it! The bulk of the work for creating and running a Docker image with an anaconda environment in Paperspace is done with the above 4 lines of code. . The entire Dockerfile is available on GitHub here. . Following Alex, I&#39;ll describe the steps to setup a Paperspace instance using this Dockerfile. . First go to your notebooks inside a project, and create a new Paperspace notebook. . . Next, choose a GPU instance to create the notebook (or CPU depending on the project), for example the RTX5000 option, and an automatic shutdown of 6 hours. . . Next the Dockerfile will have to be built and pushed to Dockerhub, before it can be used with paperspace. . To build the Dockerfile, just run . docker build -t k2rajara/paperspace:3.0 . . for example, in the same directory as the Dockerfile. . In the above code, &#39;k2rajara/paperspace&#39; is the container name on Dockerhub. To create your own repository on Dockerhub, follow the instructions here. Then push your image there. Once this is complete, its details can be added to the advanced section: . . The command that is run in the container is the default one for Paperspace: . jupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin=&#39;*&#39; --ServerApp.allow_credentials=True . Finally after clicking &quot;Start Notebook&quot;, Paperspace should pull the image from Dockerhub and run jupyter lab as requested. . In my case, the conda environment I created is available on Github here and shown here: . name: tf2 channels: - conda-forge - defaults - anaconda dependencies: - python=3.6 - pip=21 - jupyterlab - jupyter_client=7.1.0 - jupyter_server=1.13.1 - tensorflow-gpu=2.2 - pip: - mlflow==1.11.0 - python-dotenv==0.15.0 - matplotlib==3.1.2 - Pillow==8.0.1 - scikit-image==0.17.2 - ruamel_yaml==0.16.12 . The project I was working on used python 3.6, and tensorflow 2.2. Due to the outated version of python, I also had to install the jupyter client and server manually, to avoid bugs. However, the environment can be customized depending on your project requirements. . For example, to create a minimal installation with tensorflow2 and jupyterlab, use: . name: tf2 channels: - anaconda dependencies: - jupyterlab - tensorflow-gpu .",
            "url": "https://krishanr.github.io/fastpages/docker/tools/2022/06/15/anaconda-in-docker-on-paperspace.html",
            "relUrl": "/docker/tools/2022/06/15/anaconda-in-docker-on-paperspace.html",
            "date": " • Jun 15, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Principal component analysis",
            "content": "Principal component analysis (PCA) is a well known technique for dimensionality reduction, dating back over one hundred years. Here we&#39;ll summarize the theory and application of PCA, showing for example that it&#39;s most naturally applicable to multivariate normal data, which is determined by its first two moments. We shall derive PCA as a canonical form for random vectors, obtained using the population covariance matrix for a population or the sample covariance matrix for a sample. We&#39;ll also see how this decomposition differs when using the population correlation matrix or the sample correlation matrix instead. We conclude with a computation of PCA for the iris dataset, first from scratch using numpy, and then using the scikit-learn API. . We follow chapter 8 in (Johnson &amp; Wichern, 1982) for the theory of PCA, both in terms of populations of random variables and samples of random variables, and then apply it following chapter 8 in (Géron, 2017). We also follow Professor Helwig&#39;s notes available here. . By making a distinction between population and sample PCA, we can make statements about limits of the sample PCA as the sample size goes to infinity. Besides theoretical interest, these limits are useful for calculating confidence intervals, p-values, etc. . Note hereafter that all vectors are assumed to row vectors unless specified otherwise, and we&#39;ll use $ vec{1}$ to denote the row vector with components all equal to $1$. We&#39;ll also make the following definitions. Let $ vec{X} = (X_1, ..., X_p)$ be a random (row) vector. The vector has (population) mean $ vec{ mu} = mathbb{E}[ vec{X}]$ and (population) covariance matrix $ Sigma = mathbb{E}[( vec{X} - vec{ mu})^T( vec{X} - vec{ mu})] $. The (population) correlation matrix is . $$ P = begin{bmatrix} 1 &amp; rho_{12} &amp; rho_{13} &amp; dots &amp; rho_{1p} rho_{21} &amp; 1 &amp; rho_{23} &amp; dots &amp; rho_{2p} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots rho_{p1} &amp; rho_{p2} &amp; rho_{p3} &amp; dots &amp; 1 end{bmatrix} $$where . $$ begin{aligned} rho_{jk} &amp; = frac{ sigma_{jk}}{ sqrt{ sigma_{jj} sigma_{kk} }}, &amp; sigma_{jk} &amp; = Sigma_{jk} end{aligned} $$is the Pearson correlation coefficient between variables $X_j$ and $X_k$. . Transformations of random vectors . For $ vec{v}, vec{u} in mathbb{R}^p$, let $Y = vec{X} vec{v}^T = vec{X} cdot vec{v}$ and $Z = vec{X} cdot vec{u}$, then one can show that . $$ begin{aligned} mathbb{E}[Y] &amp; = vec{ mu} cdot vec{v} operatorname{cov}[Y, Z] &amp; = vec{v} : Sigma : vec{u}^T end{aligned} $$More generally, with $v_1, dotsc,v_q in mathbb{R}^p$ and $c in mathbb{R}^q$, let $Y = vec{X} cdot vec{v_i}$, and define the $q times p$ matrix $V$ by: . $$ V = begin{bmatrix} vec{v}_1 vdots vec{v}_q end{bmatrix} $$so that $ vec{Y} = vec{X} cdot V^T$, then . $$ begin{aligned} mathbb{E}[ vec{Y} + c] &amp; = vec{ mu} cdot V^T + c operatorname{cov}[ vec{Y} + c ] &amp; = operatorname{cov}[ vec{X} cdot V^T] = V ; Sigma_X ; V^T end{aligned} $$Also note that . $$ sum_{j=1}^q operatorname{Var}(Y_j) = operatorname{tr}( operatorname{cov}[ vec{Y}]) = operatorname{tr}(V ; Sigma_X ; V^T) = operatorname{tr}( Sigma_X ; V^T V) $$ . by the cyclic property of the trace. Hence when $q = p$ and $V$ is orthonormal, . $$ sum_{j=1}^p operatorname{Var}(Y_j) = operatorname{tr}( operatorname{cov}[ vec{Y}]) = operatorname{tr}(A ; Sigma_X ; A^T) = operatorname{tr}( Sigma_X ) = sum_{j=1}^p operatorname{Var}(X_j) $$ . Spectral theorem for symmetric matricies . Recall for any $p times p$ symmetric matrix $ Sigma$, if we let $ lambda_1 geq lambda_2 geq dots geq lambda_p geq 0$ be the eigenvalues of $ Sigma$, then . $$ begin{aligned} lambda_1 &amp; = operatorname{max}_{x neq 0} frac{ vec{x} : Sigma : vec{x}^T}{ lVert x rVert^2} lambda_p &amp; = operatorname{min}_{x neq 0} frac{ vec{x} : Sigma : vec{x}^T}{ lVert x rVert^2} frac{ lambda_1}{ lambda_p} &amp; = kappa( Sigma) = operatorname{max}_{x , y neq 0} frac{ vec{x} : Sigma : vec{x}^T}{ lVert x rVert^2} frac{ lVert x rVert^2}{ vec{y} : Sigma : vec{y}^T} end{aligned} $$where $ kappa( Sigma)$ is the condition number of $ Sigma$, defined when $ Sigma$ is positive definite. There are also similar formulas for the other eigenvalues defined on appropriate subspaces. . Finally, since the covariance matrix $ Sigma$ is positive semi-definite, the spectral theorem applies to it, which we shall review later. This together with the fact that any random variable with zero variance is zero implies that $ Sigma$ is positive definite iff the random variables $X_1, ..., X_p$ are linearly independent. . Two dimensions . Here&#39;s a useful corollary of the above transformation law for the covariance matrix. Suppose $X_1$ and $X_2$ are random variables with covariance matrix $ Sigma_X$. Then the variables $Y_1$ and $Y_2$ defined by . $$ begin{aligned} Y_1 &amp; = X_1 - X_2 Y_2 &amp; = X_1 + X_2 end{aligned} $$have covariance . $$ Sigma_Y = operatorname{cov}[ vec{Y}] = begin{bmatrix} sigma_{11} - 2 sigma_{12} + sigma_{22} &amp; sigma_{11} - sigma_{22} sigma_{11} - sigma_{22} &amp; sigma_{11} + 2 sigma_{12} + sigma_{22} end{bmatrix} $$Thus $Y_1$ and $Y_2$ are uncorrelated iff $X_1$ and $X_2$ have the same variance, i.e. $ sigma_{11} = sigma_{22}$. . Moreover if $X_1$ and $X_2$ are independent, then $Y_1$ and $Y_2$ are in general not independent. In fact, if $Y_1$ and $Y_2$ are also independent, then $X_1$ and $X_2$ are normal random variables (see here). This fact is known as Bernstein&#39;s theorem, and holds for more general linear combinations. . Thus a unitary transformation of iid random variables have the same covariance matrix but are not in general independent. . Population PCA . As above let $ vec{Y} = vec{X} cdot V^T$ where $V$ is a $pxp$ matrix. Then the formula . $$ Sigma_Y = operatorname{cov}[ vec{Y}] = V ; Sigma_X ; V^T $$ . shows that $ Sigma_Y$ and $ Sigma_X$ are similar quadratic forms. Thus since $ Sigma_X$ is symmetric, by the spectral theorem there exists a new set of random variables $Y_1, dotsc, Y_p$ in which $ Sigma_Y$ is diagonal. These are the principal components of the random vector $ vec{X}$, which can also be defined using notations from probability theory as follows: . $$ begin{aligned} text{First principal component } = Y_1 = &amp; text{ linear combination of } vec{X} vec{v}_1^T text{ that maximizes } &amp; operatorname{Var}[ vec{X} vec{v}_1^T] text{ subject to } lVert vec{v}_1 rVert = 1 text{Second principal component } = Y_2 = &amp; text{ linear combination of } vec{X} vec{v}_2^T text{ that maximizes } &amp; operatorname{Var}[ vec{X} vec{v}_2^T] text{ subject to } lVert vec{v}_2 rVert = 1 text{ and } vec{v_2} : Sigma : vec{v_1}^T = operatorname{Cov}[ vec{X} vec{v}_1^T, vec{X} vec{v}_2^T] = 0 &amp; vdots text{Last principal component } = Y_p = &amp; text{ linear combination of } vec{X} vec{v}_p^T text{ that maximizes } &amp; operatorname{Var}[ vec{X} vec{v}_p^T] text{ subject to } lVert vec{v}_p rVert = 1 text{ and } operatorname{Cov}[ vec{X} vec{v}_p^T, vec{X} vec{v}_1^T] = operatorname{Cov}[ vec{X} vec{v}_p^T, vec{X} vec{v}_2^T] = dots = operatorname{Cov}[ vec{X} vec{v}_p^T, vec{X} vec{v}_{p-1}^T] = 0 end{aligned} $$Equivalently, let $( lambda_1, vec{e}_1)$, $( lambda_2, vec{e}_2)$, ..., $( lambda_p, vec{e}_p)$ be the eigenvalue-eigenvector pairs of $ Sigma$ with the eigenvalues ordered in non-increasing order. Then the ith principal component is: . $$ Y_i = vec{X} vec{e}_i^T quad, i = 1,...,p $$with . $$ begin{aligned} operatorname{Var}[Y_i] &amp; = lambda_i quad i = 1,...,p operatorname{Cov}[Y_i, Y_j] &amp; = 0 quad i neq j end{aligned} $$Thus the ith maximal variance of $ Sigma$ is the ith eigenvalue of $ Sigma$. . Also note that only unique eigenvalues have unique eigenvectors (up to sign), so the principal components are in general not unique. . The above formula for the trace implies that the random variables $Y_1, ..., Y_p$ have the same total variance as $X_1, ..., X_p$, since . $$ sum_{j=1}^p operatorname{Var}(X_j) = operatorname{tr}( Sigma) = operatorname{tr}(V ; Sigma_X ; V^T ) = operatorname{tr}( Sigma_X) = sum_{j=1}^p operatorname{Var}(Y_j) = sum_{j=1}^p lambda_j $$ . Thus we can define the proportion of variance captured by the $k$th principal component as . $$ begin{aligned} &amp; text{Proporition of variance due } = frac{ lambda_k}{ sum_{j=1}^p lambda_j}, quad k= 1,...,p &amp; text{to the $k$th principal component } end{aligned} $$The basic idea behind PCA for dimensionality reduction is to only keep the the first $k$ principal components which captures, say $90 %$, of the variance, thereby reducing the number of variables. . Another way to select the principal components is to choose the most variables possible such that the conditon number (ratio of largest to smallest eigenvalues) of the reduced covariance matrix is close to $1$. This is because machine learning algorithms, like linear and logistic regression, preform better (more precision and faster convergence for gradient based methods) when the condition number of the covariance matrix is close to $1$. . Population PCA using the correlation matrix . PCA can also be calculated using the correlation matrix $P$ which is the covariance matrix of the standardized features having their mean set to $0$ and variance to $1$. Specifically, one applies the above calculations to the following variables: . $$ begin{aligned} Z_1 &amp;= frac{X_1 - mu_1}{ sqrt{ sigma_{11}}} Z_2 &amp;= frac{X_2 - mu_2}{ sqrt{ sigma_{22}}} vdots &amp; quad vdots Z_p &amp;= frac{X_p - mu_p}{ sqrt{ sigma_{pp}}} end{aligned} $$In matrix notation we have . $$ begin{aligned} vec{Z} &amp;= ( vec{W}^{ frac{1}{2}})^{-1}( vec{X} - vec{ mu}) operatorname{cov}[ vec{Z}] &amp; = ( vec{W}^{ frac{1}{2}})^{-1} ; Sigma ; ( vec{W}^{ frac{1}{2}})^{-1} = P end{aligned} $$where $ vec{W} = operatorname{diag}( sigma_{11},..., sigma_{pp})$. Then one obtains a decomposition similar to above with the following difference in the formula: . $$ sum_{j=1}^p operatorname{Var}(Y_j) = sum_{j=1}^p operatorname{Var}(X_j) = p $$Then the formula for the proportion of variance captured by the $k$th principal component simplifies to: . $$ begin{aligned} &amp; text{Proporition of variance due } = frac{ lambda_k}{p}, quad k= 1,...,p &amp; text{to the $k$th principal component } end{aligned} $$It&#39;s important to note that the principal components calculated from $ Sigma$ and $P$ can differ significantly, being a different linear combination of the original variables (Example 8.2 in (Johnson &amp; Wichern, 1982)). The principal components are similar when the variances for the $p$ random variables are the same, $ sigma = sigma_{11} = sigma_{22} = dots = sigma_{pp}$. Finally note that it&#39;s good practise to work with variables on the same scales so that the principal components aren&#39;t heavily skewed towards the larger variables. . Sample PCA . Consider $n$ iid realizations $ vec{X}_1, ..., vec{X}_n$ from the random vector $ vec{X}$. We can arrange this data into a feature matrix . $$ X = begin{bmatrix} vec{X}_1 vdots vec{X}_n end{bmatrix} $$The sample covariance matrix, $S$, is: . $$ begin{aligned} S_{i j} = frac{1}{n-1} sum_{k=1}^n (X_{k i} - bar{X}_i) (X_{k j} - bar{X}_j) = frac{1}{n-1} (X - vec{1} otimes bar{X})^T (X - bar{X} otimes vec{1}) end{aligned} quad quad i= 1,...,p text{ and } j = 1,...,p$$and the sample correlation matrix, $R$, is: . $$ begin{aligned} R_{ij} = frac{S_{ij}}{ sqrt{S_{ii}} sqrt{S_{jj}}} = frac{ sum_{k=1}^n (X_{k i} - bar{X}_i) (X_{k j} - bar{X}_j)}{ sqrt{ sum_{k=1}^n (X_{k i} - bar{X}_i)^2 } sqrt{ sum_{k=1}^n (X_{k j} - bar{X}_j)^2}} end{aligned} quad quad i= 1,...,p text{ and } j = 1,...,p $$For $ vec{v} in mathbb{R}^p$ a new feature is defined by the equation . $$ Y = vec{X} cdot vec{v} $$ . Formulas similar to above show that the sample covariance matrix transforms according to (see chapter 8 in (Johnson &amp; Wichern, 1982)): . $$ S_Y = V S_X V^T $$ . Thus an identical construction can be given for the principal components, now using $S$ in place of $ Sigma$, or $R$ in place of $P$. . Reconstruction error . Let $W_d = [ v_1 dots v_d ]$ be the matrix whose columns are the first $d$ eigenvectors of $S_X$ (ordered from largest to smallest eigenvalue). Then given any feature vector $ vec{X}_i$, the vector $ vec{X}_i W_d$ is the $d$ dimensional projection, and the vector $ vec{X}_i W_d W_d^T$ is the reconstructed vector. There&#39;s a fairly simple formula for the reconstruction error . $$ sum_{i=1}^n || vec{X}_i W_d W_d^T - vec{X}_i||^2 $$It turns out that (see chapter 9 in (Johnson &amp; Wichern, 1982) for a proof) . $$ begin{aligned} sum_{i=1}^n || vec{X}_i W_d W_d^T - vec{X}_i||^2 &amp; = operatorname{tr}[(X W_d W_d^T - X)(X W_d W_d^T - X)^T] &amp; vdots &amp; = lambda_{d+1} + dots + lambda_p end{aligned} $$Thus the recontruction error of the $d$ dimensional projection is equal to the remaining $p-d$ eigenvalues of the covariance matrix, i.e. the unexplained variance. Finally, note that this is the smallest possible error among all $d$ dimensional projections, and this is another defining property of PCA. . Large sample properties . As expected, when the feature matrix $X$ comes from a multivariate Gaussian, the principal components recover the natural frame which diagonalizes the covariance matrix of the Gaussian. In fact, much more can be said. . When $ vec{X}_i overset{iid}{ sim} N( vec{ mu}, vec{ Sigma})$ and the eigenvalues of $ Sigma$ are strictly positive and unique: $ lambda_1 &gt; dots lambda_p &gt; 0$, we can describe the large sample properties of the principal directions and eigenvalues. Then as $n rightarrow infty$, we have ((Johnson &amp; Wichern, 1982)): . $$ sqrt{n}( hat{ vec{ lambda}} - vec{ lambda}) approx N( vec{0}, 2 vec{ Lambda}^2) sqrt{n}( hat{ vec{v}}_k - vec{v}_k) approx N( vec{0}, vec{V}_k) $$where $ Lambda = operatorname{diag}( lambda_1, ..., lambda_k)$ and . $$ vec{V}_k = lambda_k sum_{l neq k} frac{ lambda_l}{( lambda_l - lambda_k)^2} vec{v}_l^T vec{v}_l $$Furthermore, as $n rightarrow infty$, we have that $ hat{ lambda}_k$ and $ hat{ vec{v}}_k$ are independent. . Practical considerations . Below we follow A. Geron&#39;s notes on PCA to implement it in numpy and sckit-learn for the iris dataset. . import pandas as pd from sklearn import datasets from sklearn.decomposition import PCA import numpy as np iris = datasets.load_iris() . print(iris[&#39;feature_names&#39;]) . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . df = pd.DataFrame(iris[&#39;data&#39;], columns=[&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]) df.head(5) . sepal_length sepal_width petal_length petal_width . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . df.describe() . sepal_length sepal_width petal_length petal_width . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | . Since the units of each feature are comparable (the standard deviations are of the same order of magnitude), we will calculate PCA using the sample covariance matrix. If on the otherhand, the magnitudes differed by orders of magnitude, we would calculate PCA using the sample correlation matrix. . PCA using SVD directly . The principal components can be calculated by diagonalizing the covariance matrix or calulating the singular value decomposition of the feature matrix. We&#39;ll use the latter for numerical reasons. . X = iris[&#39;data&#39;] . The SVD of $X$ is calculated after subtracting off the sample mean. We&#39;ll call this matrix $X_{centered}$ and work with it hereafter. Then the SVD of $X_{centered}$ is . $X_{centered} = U D V^T$ . where $U$ is an $n times n$ orthonormal matrix, $V$ is an $m times m$ orthonormal matrix, and $D = operatorname{diag}(d_{11}, ..., d_{mm})$ is an $n times m$ rectangular diagonal matrix. . The matrix $V$ is the matrix of eigenvectors from the sample PCA above, and the diagonal covariance matrix $S_Y$ is obtained by squaring the singular values and dividing by $n-1$: . $$ S_Y = frac{1}{n-1} begin{bmatrix} d_{11}^2 &amp; &amp; &amp; ddots &amp; &amp; &amp; d_{mm}^2 end{bmatrix} $$For this example, we have: . X_centered = X - X.mean(axis=0) U, D, Vt = np.linalg.svd(X_centered) V = Vt.T eigs = D ** 2 . The $d$-dimensional projection can be calcualted using the $p times d$ matrix $W_d$ whose $d$ columns are the first $d$ eigenvectors. . $X_{d-proj} = X_{centered} W_d$ . W2 = V[:, :2] # W2 can also be obtained from the sklearn package using: pca.components_.T X2D_diy = X_centered @ W2 X2D_diy.shape . (150, 2) . Obtain the reconstructed data using: . $ X_{recovered} = X_{d-proj} W_{d}^T$ . A defining property of PCA is that for any dimension $d &lt; m$, the PCA reconstruction of the data minimizes the mean squared error between the original data among all possible $d$ dimensional hyperplanes. Moreover, the square of this error is equal to the sum of the $m-d$ smaller eigenvalues, as we shall see. . X_recovered = X2D_diy @ W2.T . The squared error is then: . np.linalg.norm(X_centered - X_recovered)**2 . 15.204644359438948 . Which is equal to the sum of the two smaller eigenvalues: . sum(eigs[2:]) . 15.204644359438953 . The proportion of the total sample variance due to each sample principal component is: . $$ frac{ hat{ lambda_i}}{( sum_{i=1}^m hat{ lambda_i}) } quad quad quad i = 1,...,m $$and is given numerically by . eigs/sum(eigs) . array([0.92461872, 0.05306648, 0.01710261, 0.00521218]) . We can obtain the number of principal components to retain by either (i) retaining the first $d$ components which sum up to 95% of the total variance, or (ii) looking for an elbow in the scree plot. . import matplotlib.pyplot as plt plt.plot(range(1,len(eigs)+1), np.array(eigs.tolist())) plt.title(label=&quot;Scree plot&quot;) plt.xlabel(&quot;i&quot;) plt.ylabel(&quot;lambda_i&quot;) plt.show() . There is an elbow in the plot at $i = 2$, meaning all eigenvalues after $ lambda_1$ are relatively small and about the same size. In this case, it appears that the first principal component effectively summarize the total sample variance. . Finally, we can visualize the two dimensional projection using a scatter plot. . plt.title(&quot;PCA with 2 principal components&quot;, fontsize=14) plt.scatter(X2D_diy[:, 0], X2D_diy[:,1], c=iris[&#39;target&#39;]) plt.xlabel(&quot;$y_1$&quot;, fontsize=18) plt.ylabel(&quot;$y_2$&quot;, fontsize=18) plt.show() . PCA using scikit-learn . Now we&#39;ll calculate the PCA projection using scikit-learn. Notice that scikit-learn will mean center the data itself. . pca = PCA(n_components = 2) X2D = pca.fit_transform(X) . Instead of specifing the number of components in the call to PCA above, we can also set $ operatorname{n _components}=0.95$ to obtain the number of dimensions which capture 95% of the variance in the data. Or we could pass in no arguments to get the full SVD and plot a scree plot. . Below, we check that sklearn PCA and the one calculated with SVD directly give the same answer. . np.max(X2D - X2D_diy) . 2.748330173586095 . The non-uniquness of the PCA leads to a large difference. However, this can be fixed by inspecting the projections and adjusting: . X2D_diy[:,1] = - X2D_diy[:,1] np.max(X2D - X2D_diy) . 1.3322676295501878e-15 . The explained variance ratio holds the proportion of variance explained by the first two sample principal components, and is given by . pca.explained_variance_ratio_ . array([0.92461872, 0.05306648]) . Computational complexity . Let $n_{max} = operatorname{max}(m,n)$ and $n_{min} = operatorname{min}(m,n)$. If $n_{max} &lt; 500$ and $d$ is less than $80 %$ of $n_{min}$, then scikit-learn uses the full SVD approach which has a complexity of $O(n_{max}^2 n_{min})$, otherwise it uses randomized PCA which has a complexity of $O(n_{max}^2 d)$ (cf. here and (Géron, 2017)). . There are many more variations of PCA, and other dimensionality reduction algorithms. See Decomposing signals in components (matrix factorization problems) in the scikit-learn documentation, for example. . Johnson, R. A., &amp; Wichern, D. W. (1982). Applied Multivariate Statistical Analysis. | Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. | .",
            "url": "https://krishanr.github.io/fastpages/statistics/2022/03/30/principal-component-analysis.html",
            "relUrl": "/statistics/2022/03/30/principal-component-analysis.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Standard probability distributions",
            "content": "We&#39;ll review elementary univariate probability distributions in statistics, including how the mean/variance are derived, simple relationships, and the questions they answer, with calculations done in scipy. . This review is similar in spirit to brand name distributions and univariate distribution relationships. . import scipy.special as sp import scipy.stats as ss import numpy as np import matplotlib.pyplot as plt import seaborn as sns . The univariate probability distributions answer questions about events occuring on a discrete time interval $D_n = [0, 1, dotsc, n ]$ or a continuous time interval $I_t = [0, t]$. . In the discrete case each event could be a success with probability $p$ or a failure with probability $1-p$, with different events being independent. Then we can ask the following questions: . For fixed $n$, what is the probability of observing $k$ successes? | How many events do we have to observe before seeing $1$ success or more generally, $r$ successes? | The first question is answered by the Binomial distribution, and the second is answered by the geometric and negative binomial distributions. A concrete example of such events are coin tosses, where the outcome of each toss is independent of previous tosses, and each toss comes up as heads with probability $p = frac{1}{2}$. . In the case of a continuous time interval $I_t$ there are many types of events that can occur, but we&#39;ll study the simplest type with real world applications, the events associated with a Poisson process. Then we can ask the following questions about a Poisson process: . Within a fixed time $t$, what is the probability of observing $k$ events? | How much time do we have to wait before seeing the first event or more generally, $r$ events? | The first question is answered by the Poisson distribution, and the second is answered by the exponential and Erlang (Gamma) distributions. A concrete example of events following a Poisson process are the distribution of earthquakes in a given region. . Finally, the normal distribution will arise as the limiting distribution in the famous central limit theorem. . Moving forward, do remember that . $$ operatorname{var}[X] = mathbb{E}[(X - mathbb{E}[X] )^2] = mathbb{E}[X^2] - ( mathbb{E}[X])^2 $$ . Bernoulli distribution . Suppose X is a random variable denoting having a trial with a probability of success $p$. It gives a success, $X = 1$, with probability $p$ and failure, $X = 0$, with probability $1-p$. Then $X sim operatorname{Ber}(p)$, and . $$ mathbb{E}[X] = p, : : mathbb{E}[(X - mathbb{E}[X] )^2] = p (1-p)$$ . p = 0.4 rv = ss.bernoulli(p) mean, var, skew, kurt = rv.stats(moments=&#39;mvsk&#39;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 0.4, var 0.24, skew 0.40824829046386296, kurt -1.8333333333333337 . rv.interval(0.99) . (0.0, 1.0) . fig, ax = plt.subplots() y = np.random.binomial(1, p, size=100) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(loc=&#39;upper right&#39;) ax.set(title=&#39;Bernoulli Distribution&#39;, xlabel=&#39;y&#39;); . P(X = 1) is calculated using the probability mass function below: . rv.pmf(1) . 0.4 . Binomial distribution . When we have $n$ independent trials, each with a probability of success $p$ and failure $1-p$, then $X sim operatorname{Bin}(n, p)$ which is a sum of $n$ independent $ operatorname{Ber}(p)$ random variables. I.e. letting $X_i sim operatorname{Ber}(p)$ for $i = 1, dotsc, n$, then . $$ X = sum_{i=1}^n X_i $$ . and a straightforward calculation gives . $$ mathbb{E}[X] = n p, : : mathbb{E}[(X - mathbb{E}[X] )^2] = n p (1-p)$$ . p, n = 0.4, 10 rv = ss.binom(n, p) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 4.0, var 2.4, skew 0.12909944487358052, kurt -0.1833333333333334 . fig, ax = plt.subplots() y = np.random.binomial(n, p, size=200) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Binomial distribution&quot;, xlabel=&quot;y&quot;); . P(X = 3) is calculated using the probability mass function below: . rv.pmf(3) . 0.21499084799999976 . Do a quick check that the probability mass function is normalized: . sum (rv.pmf(i) for i in range(n+1)) . 0.9999999999999994 . Geometric distribution . Suppose independent trials, each with a probability of success $p$, are performed until success occurs. Then the number of trials until the first success occurs is a random variable $X sim operatorname{Geo}(p)$, and called a geometric random variable. Its probability mass function is: . $$ P(X= n) = (1-p)^{n-1} p, : : n= 1,2, dotsc $$ . Using the probability generating function we calculate: . $$ mathbb{E}[X] = frac{1}{p}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = frac{1-p}{p^2}$$ . p = 0.4 rv = ss.geom(p) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 2.5, var 3.749999999999999, skew 2.0655911179772892, kurt 6.2666666666666675 . fig, ax = plt.subplots() y = np.random.geometric(p, size=1000) sns.countplot(x=y, label =f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Geometric Distribution&quot;, xlabel=&quot;y&quot;); . As the above histogram hints at, the geometric distribution is the discretization of the exponential distribution, and shares the memoryless property: . $$ P(X &gt; n + k | X &gt; k) = P(X &gt; n) $$ . Negative Binomial distribution . Suppose independent trials, each with a probability of success $p$, are performed until success occurs (with the last trial being a success). Then the number of trials until $r$ successes occur is a random variable $X sim operatorname{NegBin}(r,p)$ and called a negative binomial random variable, which can be expressed as a sum of $r$ independent geometric distributions $X_i sim operatorname{Geo}(p)$ for $i = 1, dotsc, n$. . $$ X = X_1 + X_2 + dotsc + X_n $$ . The probability mass function is: . $$ P(X = k) = { k + r - 1 choose k} p^r (1-p)^{k} $$ . where $k$ is the number of failures seen before seeing a total of $r$ successes. . $$ mathbb{E}[X] = frac{r (1-p)}{p}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = r frac{(1-p)}{p^2}$$ . p = 0.4 r = 4 rv = ss.nbinom(r, p) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 5.999999999999999, var 14.999999999999996, skew 1.0327955589886446, kurt 1.5666666666666669 . fig, ax = plt.subplots() y = np.random.negative_binomial(r, p, size=1000) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Negative Binomial distribution&quot;, xlabel=&quot;y&quot;); . Poisson distribution and processes . The number of events occuring in a fixed time interval $t$, with each event occuring independently of each other and at a average constant rate $ lambda$ is described by the Poisson distribution with parameter $ mu = lambda t$, $X sim Poi( mu)$, and has the probability mass function: . $$ P(X = k) = frac{ mu^k}{k!} e^{- mu} : : k = 0,1, dotsc$$ . The standard moments are, . $$ mathbb{E}[X] = mu = lambda t, : : mathbb{E}[(X - mathbb{E}[X] )^2] = mu$$ . Note that the time till the first event in a Poisson process is described by the exponential distribution with parameter $ lambda$, and the time till the first $r$ events in a Poisson process is described by the Erlang (Gamma) distribution with shape $r$ and rate $ lambda$. See below for definitions. . Also see section 20.2 from the stats cookbook for a good summary of the Poisson process and the different distributions involved. . mu = 6 rv = ss.poisson(mu) mean, var, skew, kurt = rv.stats(moments=&quot;mvsk&quot;) print(&quot;mean {}, var {}, skew {}, kurt {}&quot;.format(mean, var, skew, kurt)) . mean 6.0, var 6.0, skew 0.408248290463863, kurt 0.16666666666666666 . fig, ax = plt.subplots() y = np.random.poisson(mu, size=1000) sns.countplot(x=y, label=f&#39;mean = {y.mean(): 0.3f}&#39;, ax=ax) ax.legend(&quot;upper right&quot;) ax.set(title=&quot;Poisson distribution&quot;, xlabel=&quot;y&quot;); . Gamma function and distribution . Recall the Gamma function, $ Gamma( alpha)$, is a function of a real variable, such that $ Gamma( alpha) = alpha!$ for $ alpha = 0, 1, 2 dotsc$. . The Gamma distribution then has the probability distribution: . $$f(x; alpha, beta) = frac{x^{ alpha-1} e^{- beta x} beta^ alpha}{ Gamma( alpha)} text{ for } x &gt; 0$$ . with $ alpha &gt;0$ called the shape and $ beta &gt;0 $ called the rate. $ theta = 1/ beta$ is also called the scale parameter. . If $X sim operatorname{Ga}( alpha, beta)$ (see (Murphy, 2012)), then the standard moments are . $$ mathbb{E}[X] = frac{ alpha}{ beta}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = frac{ alpha}{ beta^2}$$ $$ operatorname{mode}{X} = frac{( alpha-1)}{ beta} $$ . We parameterize the Gamma distribution in scipy as follows. . alpha, beta = 3, 1 rv = ss.gamma(a=alpha,scale=1/beta) . rv.interval(0.99) . (0.33786338872773347, 9.273792089255544) . fig, ax = plt.subplots(1, 1) x = np.linspace(0, rv.ppf(0.999), 100) ax.plot(x, rv.pdf(x), &#39;r-&#39;, lw=2, alpha=0.6, label=&#39;gamma pdf&#39;) ax.set(title=&quot;Gamma PDF&quot;, xlabel=&quot;y&quot;); . Exponential distribution . The Exponential, Erlang, and Chi-Squared distributions are all related to the Gamma distribution (Murphy, 2012), but we&#39;ll describe the simplest, the exponential distribution. . $$ operatorname{Exp}(x | lambda) = operatorname{Ga}(x | 1, lambda)$$ . where $ lambda$ is the rate parameter. The exponential distribution describes the waiting time between events in a Poisson process. . From the above formulas, if $X sim operatorname{Exp}(x | lambda)$, then . $$ mathbb{E}[X] = frac{1}{ lambda}, : : mathbb{E}[(X - mathbb{E}[X] )^2] = frac{1}{ lambda^2}$$ $$ operatorname{mode}{X} = 0 $$ . The exponential distribution, which is the continuous analog of the geometric distribution, also has the memoryless property: . $$ P(X &gt; t + h | X &gt; h) = P(X &gt; t )$$ . for any $t, h &gt; 0 $. . Characteristic functions of random variables . The charactierstic function will be useful for working with these distributions. . The characteristic function (also called the Fourier transform) of a random variable $X$ is . $$ phi(t) = mathbb{E}[e^{i t X}] : : text{ for } t in mathbb{R}$$ . This function satisfies $| phi(t)| &lt;= 1$ for all $t$ and is uniformly continuous. It has some advantages over the moment generating function, including the fact that it&#39;s defined everywhere, and that it uniquely determines the random variable (Grimmett &amp; Stirzaker, 2001). . Some other facts that we can derive from characteristic functions: . If $ phi^k(0)$ and $ mathbb{E}[|X^k|] &lt; infty$, then | $$ phi^k(0) = i^k mathbb{E}[X^k] $$ . If $X_1, X_2, dotsc, X_n$ are independent then | $$ phi_{X_1 + dotsc + X_n}(t) = phi_{X_1}(t) dots phi_{X_n}(t) $$ . If $a, b in mathbb{R}$ and $Y = a X + b $ then | $$ phi_{Y}(t) = e^{i t b } phi_{a X}(t) $$ . For random variables $X$ and $Y$, define the join characteristic function of $X$, and $Y$ by $ phi_{X, Y}(s,t) = mathbb{E}[e^{i s X} e^{i t Y}]$. Then $X$ and $Y$ are independent iff | $$ phi_{X, Y}(s,t) = phi_X(s) phi_Y(t) text{ for all } s, t in mathbb{R} $$ . See chapter 5 of (Grimmett &amp; Stirzaker, 2001) for the statement and proof of the above facts. . Examples of characteristic functions . If $X sim operatorname{Ber}(p)$ then . $$ phi(t) = 1 - p + p e^{i t} $$ . If $X sim operatorname{Bin}(n,p)$ then . $$ phi(t) = (1 - p + p e^{i t} )^n $$ . If $X sim operatorname{NegBin}(r,p)$ then . $$ phi(t) = left ( frac{p}{1 - e^{it} + p e^{i t}} right )^r $$ . If $X sim operatorname{Gam}( alpha, beta)$ then . $$ phi(t) = left ( frac{ beta}{ beta - i t} right )^ alpha $$ . If $X sim operatorname{Exp}( lambda)$ then . $$ phi(t) = frac{ lambda}{ lambda - i t} $$ . If $X sim operatorname{Poi}( mu)$ then . $$ phi(t) = e^{ mu ( e^{i t} - 1)}$$ . If $X sim operatorname{N}( mu, sigma)$ then . $$ phi(t) = exp( i mu t - frac{ sigma t^2}{2}) $$ . If $X sim U(a, b)$ (continuous Uniform random variable) then . $$ phi(t) = frac{e^{i t b} - e^{i t a}}{ i t (b - a)} $$ . See section 5.8 in (Grimmett &amp; Stirzaker, 2001) for more details. Those functions, plus the above facts about characteristic functions, and the following . $$ operatorname{var}[X] = mathbb{E}[X^2] - ( mathbb{E}[X])^2 $$ . can be used to calculate $ mathbb{E}[X]$ and $ operatorname{var}[X]$. . The above formulas also clearly show which distributions are closed under taking sums of independent copies. . Normal distribution, weak law of large numbers, and the central limit theorem . We&#39;ll now study the normal distribution as the error distribution in the central limit theorem. . First recall that the normal distribution has the probability distribution: . $$ f(x) = frac{1}{ sigma sqrt{2 pi} } e^{- frac{1}{2} left ( frac{x- mu}{ sigma} right)^2}$$ . where $ mu in mathbb{R}$ and $ sigma &gt; 0$, and the standard moments are . $$ mathbb{E}[X] = mu, : : mathbb{E}[(X - mathbb{E}[X] )^2] = sigma$$ $$ operatorname{mode}{X} = mu $$ . Now consider a sequence $X_1$, $X_2$, ... of independent and identically distributed random variables each with mean $ mu$ and standard deviation $ sigma$ (good examples being the Bernoulli and Exponential distributions). Let $S_n = X_1 + X_2 + dots + X_n$ be their partial sums, then the weak law of large numbers1 says that $ frac{S_n}{n}$ converges to $ mu$ in distribution as $n$ approaches $ infty$ (see section 5.10 in (Grimmett &amp; Stirzaker, 2001)). We&#39;ll illustrate this theorem by way of example below. . lam = 0.5 mu = 2 def draw(n): return np.random.exponential(mu, size=n) . Below we&#39;ll plot the distribution of $ frac{S_n}{n}$ for $n = 1, 100$ and $1000$ to see the weak law in action. . fig = plt.figure(figsize=(10,10)) n = 1 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(221) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(&quot;Distribution of a random variable&quot;) n = 100 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(222) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Average of {n} IID random variables&quot;) n = 1000 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(223) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Average of {n} IID random variables&quot;) fig.suptitle(&#39;Illustration of the Weak Law of Large Numbers&#39;) plt.show() . It clearly looks like the errors in $ frac{S_n}{n} - mu$ are approaching a normal distribution, in fact $(S_n - n mu)/ sqrt{n sigma^2}$ approaches $N(0,1)$ in distribution as $n$ approaches $ infty$ (provided the $X_i$s has finite non-zero variance, (Grimmett &amp; Stirzaker, 2001)). This fact is the famous central limit theorem, which we&#39;ll illustrate below. . fig = plt.figure(figsize=(10,10)) n = 1 y = np.array([ sum(draw(n))/n for i in range(1000) ] ) ax = fig.add_subplot(221) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(&quot;Distribution of a random variable&quot;) n = 100 y = np.array([ (sum(draw(n)) - mu * n)/np.sqrt(n) for i in range(1000) ] ) ax = fig.add_subplot(222) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Errors of {n} IID random variables&quot;) n = 1000 y = np.array([ (sum(draw(n)) - mu * n)/np.sqrt(n) for i in range(1000) ] ) ax = fig.add_subplot(223) ax.hist(y, label=f&#39;mean = {y.mean(): 0.3f}&#39;) ax.title.set_text(f&quot;Errors of {n} IID random variables&quot;) fig.suptitle(&#39;Illustration of the Central Limit Theorem&#39;) plt.show() . Note that it&#39;s straightforward to check that . $$ mathbb{E}[(S_n - n mu)/ sqrt{n sigma^2}] = 0, : : operatorname{var}[(S_n - n mu)/ sqrt{n sigma^2}] = 1$$ . for all $ n &gt; 0 $. So the first two moments of $(S_n - n mu)/ sqrt{n sigma^2}$ agree with the first two moments of $N(0, 1)$. The central limit theorem then says that in the limit of large $n$, all moments of $(S_n - n mu)/ sqrt{n sigma^2}$ agree with all moments of $N(0, 1)$. . Observe that these theorems naturally apply to the Bernoulli and Exponential distributions in the limit of large $n$, showing that after setting the mean to zero and scaling by the standard deviation, they both converge to the standard normal distribution. . Finally, note that the weak law of large numbers, and the central limit theorem can be proven fairly elegantly using charactierstic functions (see (Grimmett &amp; Stirzaker, 2001), for example). Also see chapter 15 in Introduction to probability at an advanced level for more applications of the central limit theorem. . More summaries of standard statistics can be found in stats cookbook. . 1. The strong law of large numbers refers to almost everywhere convergence of the random variables.↩ . Murphy, K. P. (2012). Machine learning - a probabilistic perspective. | Grimmett, G. R., &amp; Stirzaker, D. R. (2001). Probability and random processes. | .",
            "url": "https://krishanr.github.io/fastpages/statistics/2022/01/13/standard-probability-distributions.html",
            "relUrl": "/statistics/2022/01/13/standard-probability-distributions.html",
            "date": " • Jan 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". My name is Krishan Rajaratnam and I’m data scientist and mathematician. I’ve always had a strong interest in mathematics, science, and technology, and I’m able to combine many of these interests in a career applying machine learning. . I finished my PhD at the University of Toronto under Professor Israel Michael Sigal studying the mathematical theory behind the Fractional Quantum Hall Effect (thesis available here). I also did a Master’s thesis at the University of Waterloo under Professor Dong Eui Chang and Professor Raymond G. McLenaghan studying and developing the mathematical theory behind the orthogonal separation of the Hamilton-Jacobi equation on spaces of constant curvature (thesis available here). . Besides my technical interests, I generally have a growth mindset and am interested in staying fit, with hobbies like biking, running, and weight lifting as well as photography. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://krishanr.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://krishanr.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}